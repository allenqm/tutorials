{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
    "returns the ``output``.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    ".. figure:: /_static/img/mnist.png\n",
    "   :alt: convnet\n",
    "\n",
    "   convnet\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Define the network\n",
    "------------------\n",
    "\n",
    "Let’s define this network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6x6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2,2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) #QQ: unravel the tensor so it can be fed to a dense net\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #QQ: need to turn int oa sigmoid for classification, or something else for regression, or something else for many labels?\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1042, -0.0491,  0.0138,  0.1085,  0.0212,  0.1046,  0.0260,  0.0189,\n",
      "         -0.0663,  0.0476]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0183, -0.0361, -0.0030, -0.0803,  0.0554, -0.0263, -0.0455,  0.1384,\n",
      "        -0.0291, -0.1259], grad_fn=<SelectBackward>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 32, 32)\n",
    "x = x = F.relu(net.conv1(x))\n",
    "x = F.max_pool2d(x, (2,2))\n",
    "\n",
    "x = F.relu(net.conv2(x))\n",
    "x = F.max_pool2d(x, 2)\n",
    "x = x.view(-1, net.num_flat_features(x))\n",
    "x = F.relu(net.fc1(x))\n",
    "x = F.relu(net.fc2(x))\n",
    "x = net.fc3(x)\n",
    "\n",
    "print(x[0], x.shape)\n",
    "\n",
    "# x = net.conv2(x)\n",
    "# print(x[0], x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0023,  0.0020,  0.0834, -0.0199, -0.1150, -0.0363,  0.0750, -0.0309,\n",
      "         0.0785,  0.0283], grad_fn=<SelectBackward>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Max pooling over a (2, 2) window\n",
    "x = torch.randn(1, 1, 32, 32)\n",
    "x = F.max_pool2d(F.relu(net.conv1(x)), (2, 2))\n",
    "# If the size is a square you can only specify a single number\n",
    "x = F.max_pool2d(F.relu(net.conv2(x)), 2)\n",
    "x = x.view(-1, net.num_flat_features(x))\n",
    "x = F.relu(net.fc1(x))\n",
    "x = F.relu(net.fc2(x))\n",
    "x = net.fc3(x)\n",
    "print(x[0], x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0777, -0.1345,  0.2179],\n",
       "          [-0.0741, -0.2793, -0.1412],\n",
       "          [ 0.3285, -0.1576,  0.1425]]],\n",
       "\n",
       "\n",
       "        [[[-0.0036,  0.0382, -0.1393],\n",
       "          [ 0.2593,  0.1974,  0.1967],\n",
       "          [-0.1425,  0.2781, -0.2777]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0796,  0.2798, -0.2864],\n",
       "          [ 0.2809,  0.1457,  0.0103],\n",
       "          [-0.1299,  0.0395, -0.1966]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1813, -0.2717,  0.2518],\n",
       "          [-0.1234, -0.3090, -0.1879],\n",
       "          [ 0.1291,  0.1899,  0.3126]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2703,  0.1953,  0.2264],\n",
       "          [ 0.0842, -0.3084,  0.0964],\n",
       "          [-0.0077,  0.2323,  0.1986]]],\n",
       "\n",
       "\n",
       "        [[[-0.1336, -0.1257, -0.0574],\n",
       "          [ 0.2781,  0.1996,  0.0563],\n",
       "          [ 0.1192,  0.3174, -0.0979]]]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0777, -0.1345,  0.2179],\n",
       "          [-0.0741, -0.2793, -0.1412],\n",
       "          [ 0.3285, -0.1576,  0.1425]]],\n",
       "\n",
       "\n",
       "        [[[-0.0036,  0.0382, -0.1393],\n",
       "          [ 0.2593,  0.1974,  0.1967],\n",
       "          [-0.1425,  0.2781, -0.2777]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0796,  0.2798, -0.2864],\n",
       "          [ 0.2809,  0.1457,  0.0103],\n",
       "          [-0.1299,  0.0395, -0.1966]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1813, -0.2717,  0.2518],\n",
       "          [-0.1234, -0.3090, -0.1879],\n",
       "          [ 0.1291,  0.1899,  0.3126]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2703,  0.1953,  0.2264],\n",
       "          [ 0.0842, -0.3084,  0.0964],\n",
       "          [-0.0077,  0.2323,  0.1986]]],\n",
       "\n",
       "\n",
       "        [[[-0.1336, -0.1257, -0.0574],\n",
       "          [ 0.2781,  0.1996,  0.0563],\n",
       "          [ 0.1192,  0.3174, -0.0979]]]], requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random 32x32 input.\n",
    "Note: expected input size of this net (LeNet) is 32x32. To use this net on\n",
    "the MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0157, -0.0312, -0.0706,  0.0180,  0.0730,  0.0787,  0.1607,  0.0893,\n",
      "          0.0633,  0.0396]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "    a fake batch dimension.</p></div>\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation creates at\n",
    "     least a single ``Function`` node that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "`loss functions <https://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6999, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5546, -1.0413,  1.0663,  1.3701,  1.7462, -1.5311,  0.1164, -0.3960,\n",
       "         -0.4327,  0.1205]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.view(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5379, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7fd38c67dc50>, 0),\n",
       " (<ReluBackward0 at 0x7fd38c67d3d0>, 0),\n",
       " (<TBackward at 0x7fd38c67d490>, 0))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0].next_functions[1][0].next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formTree(lst): \n",
    "    tree = {} \n",
    "    for item[0] in lst: \n",
    "        currTree = tree \n",
    "  \n",
    "        for key in item[0][::-1]: \n",
    "            if key not in currTree: \n",
    "                currTree[key] = {} \n",
    "            currTree = currTree[key] \n",
    "              \n",
    "    return tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fd38c593210>\n",
      "<AddmmBackward object at 0x7fd38dfcb550>\n",
      "<AccumulateGrad object at 0x7fd38df4ad50>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([0.0042, 0.0017, 0.0017, 0.0086, 0.0037, 0.0181])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is `here <https://pytorch.org/docs/nn>`_.\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple Python code:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note::\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in the `Backprop`_ section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0045, -0.0216,  0.0069],\n",
       "          [-0.0107,  0.0131,  0.0012],\n",
       "          [ 0.0085, -0.0075,  0.0168]]],\n",
       "\n",
       "\n",
       "        [[[-0.0244,  0.0009,  0.0093],\n",
       "          [-0.0047,  0.0057,  0.0020],\n",
       "          [ 0.0007, -0.0058, -0.0223]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0025,  0.0107, -0.0040],\n",
       "          [-0.0129,  0.0036, -0.0064],\n",
       "          [-0.0106,  0.0052,  0.0075]]],\n",
       "\n",
       "\n",
       "        [[[-0.0053, -0.0122,  0.0073],\n",
       "          [ 0.0124, -0.0010, -0.0142],\n",
       "          [ 0.0171,  0.0053, -0.0016]]],\n",
       "\n",
       "\n",
       "        [[[-0.0005, -0.0100,  0.0043],\n",
       "          [ 0.0035,  0.0067, -0.0035],\n",
       "          [-0.0010, -0.0128, -0.0115]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0124, -0.0132,  0.0026],\n",
       "          [ 0.0240, -0.0012, -0.0280],\n",
       "          [-0.0062, -0.0054,  0.0088]]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())[0].grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1708, -0.1631,  0.1065],\n",
      "          [ 0.0154, -0.2791, -0.0735],\n",
      "          [-0.3141, -0.0578, -0.0067]]],\n",
      "\n",
      "\n",
      "        [[[-0.0773,  0.0713, -0.2849],\n",
      "          [-0.0330,  0.0992,  0.3285],\n",
      "          [-0.2101, -0.2138, -0.0871]]],\n",
      "\n",
      "\n",
      "        [[[-0.0972, -0.2907,  0.0486],\n",
      "          [-0.1545,  0.2436, -0.1192],\n",
      "          [-0.0904,  0.2773, -0.1412]]],\n",
      "\n",
      "\n",
      "        [[[-0.2057, -0.2692,  0.1554],\n",
      "          [-0.1270,  0.0954, -0.0358],\n",
      "          [ 0.1113,  0.1724, -0.1953]]],\n",
      "\n",
      "\n",
      "        [[[-0.2750, -0.2322, -0.1473],\n",
      "          [-0.2119,  0.3159,  0.1333],\n",
      "          [-0.1316,  0.2882,  0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.1392, -0.0272,  0.1848],\n",
      "          [ 0.1091, -0.0081, -0.1786],\n",
      "          [-0.0239, -0.2933,  0.0404]]]])\n",
      "tensor([ 0.1022,  0.2704,  0.1789,  0.0725, -0.2507,  0.1652])\n",
      "tensor([[[[ 6.3000e-02,  5.3595e-02,  1.0445e-01],\n",
      "          [ 7.1985e-02, -1.0692e-01,  7.3201e-02],\n",
      "          [-1.0170e-01,  7.3178e-02,  1.1409e-01]],\n",
      "\n",
      "         [[-1.2288e-01, -1.1253e-01, -1.1343e-01],\n",
      "          [-3.5275e-02,  3.2203e-02,  6.2725e-02],\n",
      "          [ 1.0469e-01,  3.8222e-03, -3.0125e-02]],\n",
      "\n",
      "         [[ 8.4791e-02,  3.3805e-02, -1.1437e-01],\n",
      "          [ 1.2555e-01,  6.1211e-02,  2.9192e-02],\n",
      "          [-3.5489e-02, -5.1509e-03,  6.6417e-02]],\n",
      "\n",
      "         [[ 1.8010e-02,  1.2768e-01,  9.4131e-02],\n",
      "          [ 2.9231e-02,  1.0279e-01,  1.6666e-02],\n",
      "          [-3.2967e-02,  1.2307e-01,  7.1372e-02]],\n",
      "\n",
      "         [[ 2.5151e-02,  9.3005e-02, -7.4847e-02],\n",
      "          [ 8.5599e-02, -7.3038e-02, -1.1934e-01],\n",
      "          [ 6.4332e-02,  1.0885e-01,  6.4304e-02]],\n",
      "\n",
      "         [[-2.1577e-02,  7.8000e-02, -1.9054e-02],\n",
      "          [-7.4139e-02, -1.6317e-02, -2.8450e-02],\n",
      "          [ 1.1733e-01, -3.5410e-02, -2.0553e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1482e-02,  2.4068e-02, -1.1943e-01],\n",
      "          [ 1.0380e-01,  6.8344e-02, -9.2626e-02],\n",
      "          [ 6.5947e-02,  7.3776e-02,  2.9635e-02]],\n",
      "\n",
      "         [[ 2.2462e-02, -1.1462e-01, -4.9676e-02],\n",
      "          [ 7.6921e-02,  7.5948e-02,  4.3637e-02],\n",
      "          [-5.0181e-02, -5.3118e-02, -4.6350e-02]],\n",
      "\n",
      "         [[ 1.7252e-02,  7.2210e-02,  5.3330e-02],\n",
      "          [-5.4250e-02,  8.5923e-02, -8.9855e-02],\n",
      "          [ 7.3646e-02,  3.1518e-02, -6.7838e-02]],\n",
      "\n",
      "         [[-2.4636e-02,  1.0337e-01,  4.0272e-02],\n",
      "          [-8.8976e-02,  1.1700e-01, -1.2081e-01],\n",
      "          [-2.0530e-02, -9.6804e-02,  3.9999e-02]],\n",
      "\n",
      "         [[-5.5785e-02, -9.2789e-02, -2.0870e-02],\n",
      "          [ 8.8393e-02,  1.1213e-01,  8.7894e-02],\n",
      "          [ 6.1129e-02,  4.7303e-02,  1.1964e-01]],\n",
      "\n",
      "         [[ 1.2568e-01,  1.2970e-01,  6.7102e-02],\n",
      "          [-3.1224e-02,  5.6833e-02, -3.9784e-02],\n",
      "          [-8.6493e-02, -6.0680e-02, -5.8920e-03]]],\n",
      "\n",
      "\n",
      "        [[[-6.1869e-02,  1.2290e-01, -5.0173e-02],\n",
      "          [-3.3036e-02,  6.4025e-02,  1.3068e-01],\n",
      "          [ 1.7316e-02, -7.8552e-02, -1.0370e-01]],\n",
      "\n",
      "         [[ 4.4654e-02, -1.1684e-01, -6.1617e-02],\n",
      "          [-2.3378e-02,  1.0898e-01,  3.1833e-02],\n",
      "          [ 7.0536e-02,  4.4327e-02, -1.3607e-01]],\n",
      "\n",
      "         [[ 6.6148e-02,  8.9126e-03, -1.1332e-02],\n",
      "          [-3.6970e-02, -1.1365e-01, -6.2707e-02],\n",
      "          [ 8.0351e-02, -1.9762e-02,  6.9383e-02]],\n",
      "\n",
      "         [[-2.5568e-02,  1.0248e-01,  2.9666e-02],\n",
      "          [ 9.5740e-02, -1.3569e-01, -1.0764e-01],\n",
      "          [-4.5239e-03,  7.2599e-02, -5.7018e-02]],\n",
      "\n",
      "         [[ 8.7560e-02,  6.7933e-02, -5.8279e-02],\n",
      "          [-5.8393e-02, -1.1034e-01,  8.3043e-03],\n",
      "          [-1.3352e-01, -3.8677e-02,  5.2104e-02]],\n",
      "\n",
      "         [[ 8.3274e-02,  5.0913e-02, -1.3241e-01],\n",
      "          [ 1.2678e-01, -1.1318e-01, -6.0406e-02],\n",
      "          [ 4.1693e-02, -9.9840e-02,  1.0179e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.7354e-02,  6.7493e-02, -5.3710e-02],\n",
      "          [ 9.5198e-02, -7.1678e-02, -9.5243e-02],\n",
      "          [ 3.5332e-02, -2.1927e-02,  8.2262e-02]],\n",
      "\n",
      "         [[ 1.3586e-01, -1.3058e-01, -2.5395e-02],\n",
      "          [-7.6355e-02, -4.3618e-02,  8.9635e-02],\n",
      "          [ 7.2824e-02, -4.1459e-03, -1.1015e-01]],\n",
      "\n",
      "         [[-4.0202e-02,  1.3291e-01, -4.7596e-02],\n",
      "          [ 6.8981e-02,  7.5233e-02,  7.4984e-02],\n",
      "          [-5.9178e-02, -1.8760e-02,  1.3171e-01]],\n",
      "\n",
      "         [[-5.8144e-03, -5.3206e-02, -2.3863e-02],\n",
      "          [ 1.2232e-01, -9.9367e-02,  7.5621e-02],\n",
      "          [-1.3444e-01, -1.2350e-01,  1.1012e-01]],\n",
      "\n",
      "         [[ 4.5572e-02,  4.4442e-02, -7.4868e-02],\n",
      "          [ 9.5638e-02, -1.3139e-01, -7.7289e-02],\n",
      "          [-1.0720e-01, -1.3282e-01, -5.5517e-02]],\n",
      "\n",
      "         [[-1.2248e-01, -9.9754e-02, -1.7124e-02],\n",
      "          [ 2.4130e-02, -1.1071e-02,  1.3558e-01],\n",
      "          [-1.1247e-01, -8.1393e-02, -8.4845e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.5387e-02, -9.8276e-02,  5.8013e-02],\n",
      "          [ 1.2182e-02, -6.2769e-02, -1.1187e-01],\n",
      "          [ 1.0497e-01, -8.9409e-02, -2.9829e-02]],\n",
      "\n",
      "         [[-7.0922e-02,  1.3300e-01,  1.1205e-01],\n",
      "          [-4.0703e-02, -5.1959e-02,  3.5709e-02],\n",
      "          [ 9.4559e-02, -1.0018e-01,  6.6897e-02]],\n",
      "\n",
      "         [[ 4.5876e-02, -1.2615e-01, -1.0548e-01],\n",
      "          [ 9.8760e-02, -8.0881e-02, -3.0879e-03],\n",
      "          [ 1.0365e-01, -6.4204e-02, -7.0517e-02]],\n",
      "\n",
      "         [[-3.6735e-02,  7.8687e-02,  1.3701e-02],\n",
      "          [ 1.3433e-01, -8.8976e-02,  1.2132e-03],\n",
      "          [ 4.9872e-02,  9.3396e-02,  3.7867e-02]],\n",
      "\n",
      "         [[ 8.3085e-02, -1.3532e-01, -1.5588e-02],\n",
      "          [ 7.1178e-02, -8.9289e-02,  8.5717e-02],\n",
      "          [-2.0673e-02, -9.0261e-02, -5.5866e-02]],\n",
      "\n",
      "         [[ 3.2853e-02, -2.9462e-02,  1.1192e-01],\n",
      "          [ 4.6630e-02,  1.0163e-01, -7.1430e-02],\n",
      "          [ 6.6649e-02, -1.8346e-02,  8.3011e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.0298e-02, -5.6744e-02, -9.2691e-02],\n",
      "          [-8.7330e-03, -1.5033e-02, -3.7511e-02],\n",
      "          [-1.2076e-01,  1.1455e-01,  6.7137e-02]],\n",
      "\n",
      "         [[-4.5984e-02,  3.8719e-02,  5.2348e-02],\n",
      "          [ 1.2859e-01,  1.3318e-02, -3.7694e-02],\n",
      "          [ 1.0836e-01, -2.7482e-02, -2.5745e-03]],\n",
      "\n",
      "         [[ 5.0844e-02, -2.3088e-02, -6.3729e-02],\n",
      "          [-1.2149e-01,  9.0394e-02,  1.2426e-01],\n",
      "          [ 4.4210e-02, -4.2876e-02, -7.1609e-02]],\n",
      "\n",
      "         [[-5.8173e-02, -9.5091e-02, -6.5488e-02],\n",
      "          [-7.4100e-02,  1.9381e-02,  3.6299e-02],\n",
      "          [-7.2689e-02,  2.4827e-02,  1.2992e-01]],\n",
      "\n",
      "         [[ 6.5800e-03, -1.1159e-01, -9.7412e-02],\n",
      "          [ 6.5129e-02, -9.9663e-02,  1.3046e-01],\n",
      "          [ 1.0698e-02,  4.9788e-03, -5.9069e-02]],\n",
      "\n",
      "         [[-1.2679e-01,  1.1069e-01,  3.5387e-02],\n",
      "          [-1.1040e-01,  8.5380e-02,  5.7832e-03],\n",
      "          [ 1.2995e-01,  5.9936e-02, -3.5151e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1580e-02,  1.7603e-02, -1.3595e-01],\n",
      "          [-9.9648e-02,  6.0130e-02, -5.2124e-02],\n",
      "          [ 1.0710e-02,  1.2937e-01, -1.3432e-03]],\n",
      "\n",
      "         [[ 4.2537e-02, -2.6281e-02,  3.5716e-02],\n",
      "          [-8.2155e-04, -1.0171e-01, -5.8507e-02],\n",
      "          [-1.1824e-02,  1.2143e-02, -2.6311e-02]],\n",
      "\n",
      "         [[-9.1070e-02,  7.3738e-02, -1.1934e-01],\n",
      "          [ 6.3299e-03, -8.2830e-02, -1.1401e-01],\n",
      "          [-2.5381e-02,  2.3025e-02,  5.0704e-02]],\n",
      "\n",
      "         [[-1.1986e-01, -1.3159e-01,  8.1466e-02],\n",
      "          [-9.0916e-02,  8.8790e-02,  1.2775e-01],\n",
      "          [-3.3603e-02,  3.2111e-02,  1.1890e-01]],\n",
      "\n",
      "         [[ 3.8899e-02,  1.2079e-01,  1.6713e-02],\n",
      "          [-4.5085e-02, -2.0790e-02, -7.3365e-02],\n",
      "          [ 9.2761e-02,  3.2017e-02, -6.6452e-02]],\n",
      "\n",
      "         [[-6.2732e-02, -3.2428e-03,  3.3524e-03],\n",
      "          [-6.3982e-02, -1.1597e-01, -2.4612e-02],\n",
      "          [ 2.5042e-02, -7.0295e-02,  4.3787e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0666e-02, -8.0017e-02, -1.1441e-01],\n",
      "          [-6.5333e-02,  2.9997e-02, -1.1576e-01],\n",
      "          [ 4.8246e-02,  8.0120e-02,  1.2973e-01]],\n",
      "\n",
      "         [[-4.2237e-02,  1.3913e-02,  6.1219e-02],\n",
      "          [-1.2810e-01,  3.6236e-03, -4.0084e-02],\n",
      "          [-4.5168e-03, -3.0838e-02, -6.6001e-02]],\n",
      "\n",
      "         [[-3.5114e-02, -1.1292e-01,  5.3650e-02],\n",
      "          [ 5.1421e-02,  6.4177e-02,  1.2443e-01],\n",
      "          [-1.2503e-01,  2.6277e-02,  1.3518e-01]],\n",
      "\n",
      "         [[-1.2534e-01, -2.6617e-02,  1.2072e-01],\n",
      "          [-5.9986e-02, -2.6487e-02,  8.3480e-02],\n",
      "          [ 5.8142e-02, -5.4993e-02, -4.6827e-02]],\n",
      "\n",
      "         [[ 1.0584e-01, -5.4293e-02,  2.8703e-02],\n",
      "          [-4.2920e-02, -1.1462e-01,  1.2558e-01],\n",
      "          [ 5.6846e-02, -9.0447e-02,  7.1356e-02]],\n",
      "\n",
      "         [[-1.3280e-01, -1.7373e-02,  8.6948e-02],\n",
      "          [ 1.0335e-02, -4.7537e-02, -1.8866e-02],\n",
      "          [ 1.0972e-01,  5.3194e-02,  7.8933e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0335e-01,  1.1092e-01,  2.4352e-02],\n",
      "          [-3.1169e-02,  1.3363e-01, -1.0954e-02],\n",
      "          [ 1.3009e-01,  4.2999e-02,  1.9662e-02]],\n",
      "\n",
      "         [[ 7.9876e-02,  8.4403e-02,  7.7020e-02],\n",
      "          [-5.7420e-02,  2.2239e-02, -8.4161e-02],\n",
      "          [-6.7200e-02, -1.2134e-01, -1.0007e-01]],\n",
      "\n",
      "         [[ 6.5079e-02, -1.1138e-01,  6.8922e-02],\n",
      "          [-1.7045e-02, -1.6571e-02,  8.8549e-02],\n",
      "          [-1.0590e-01,  8.3514e-02, -2.9665e-02]],\n",
      "\n",
      "         [[ 3.4810e-03, -7.9314e-02,  2.7266e-02],\n",
      "          [ 2.8735e-03, -7.2517e-02, -7.8639e-02],\n",
      "          [-5.2865e-02, -1.0940e-02, -1.2097e-03]],\n",
      "\n",
      "         [[-8.5952e-02,  8.7043e-02, -7.6079e-02],\n",
      "          [-2.0797e-02,  1.0123e-01, -6.5406e-02],\n",
      "          [-3.0051e-02, -1.1256e-01,  5.1242e-03]],\n",
      "\n",
      "         [[ 8.4662e-02,  4.5255e-03,  8.7045e-02],\n",
      "          [ 8.7324e-02,  6.9615e-03, -1.1750e-01],\n",
      "          [-6.4216e-02,  1.0700e-01, -1.1429e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1294e-01, -1.2808e-01,  2.3002e-02],\n",
      "          [-7.6984e-02,  9.5894e-03,  1.3444e-01],\n",
      "          [ 1.0016e-01,  7.6741e-02, -5.2526e-02]],\n",
      "\n",
      "         [[ 8.2912e-02, -4.6206e-02, -9.6988e-02],\n",
      "          [-5.1748e-02, -5.6918e-02, -1.0692e-01],\n",
      "          [ 6.3223e-02,  1.1358e-01, -6.2160e-02]],\n",
      "\n",
      "         [[-8.6490e-02,  5.8360e-02, -1.0063e-02],\n",
      "          [-2.7373e-02, -4.0469e-02, -5.2735e-02],\n",
      "          [ 3.1249e-02,  1.0121e-01,  2.1615e-02]],\n",
      "\n",
      "         [[-3.3756e-02, -1.1120e-01, -2.0345e-02],\n",
      "          [-5.6256e-02,  1.0045e-01, -1.2709e-01],\n",
      "          [-1.0503e-02,  1.7880e-02,  1.0198e-01]],\n",
      "\n",
      "         [[ 1.1235e-01, -8.0410e-02,  7.7810e-02],\n",
      "          [-2.2204e-02, -2.8778e-02, -1.9066e-02],\n",
      "          [ 1.2786e-01,  3.0055e-02,  3.9988e-02]],\n",
      "\n",
      "         [[ 2.4168e-02,  1.3486e-01, -1.2767e-01],\n",
      "          [ 1.2385e-01, -1.1459e-01,  7.6212e-02],\n",
      "          [-1.6147e-02,  8.9756e-02, -3.6991e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2179e-01, -3.4848e-02,  1.3398e-01],\n",
      "          [-6.7997e-02,  8.0797e-03, -9.4917e-02],\n",
      "          [ 1.0494e-02,  1.1551e-01, -8.5101e-02]],\n",
      "\n",
      "         [[-3.4315e-02,  9.4762e-02, -1.3605e-01],\n",
      "          [-7.4639e-02, -5.1789e-02,  8.3917e-02],\n",
      "          [ 1.0561e-02, -8.1711e-02, -1.0631e-01]],\n",
      "\n",
      "         [[ 4.7397e-02, -1.3399e-01, -1.3575e-01],\n",
      "          [-1.0146e-01, -9.5274e-02, -5.4048e-04],\n",
      "          [-1.0730e-02,  7.4104e-02, -1.1358e-01]],\n",
      "\n",
      "         [[ 8.9658e-02,  1.1568e-01, -1.4791e-02],\n",
      "          [-7.0305e-03, -1.2605e-01, -1.0033e-01],\n",
      "          [-5.8258e-02,  8.3695e-02, -7.3947e-02]],\n",
      "\n",
      "         [[-8.8222e-02, -3.7992e-02,  1.0982e-01],\n",
      "          [-1.1410e-01, -3.7597e-02, -2.9307e-02],\n",
      "          [ 1.3550e-01, -1.3126e-01, -8.4462e-02]],\n",
      "\n",
      "         [[ 1.3038e-01, -1.1106e-01,  3.2206e-02],\n",
      "          [-2.5885e-03, -7.3670e-02,  1.3168e-02],\n",
      "          [-1.8489e-02,  1.0029e-02,  1.3282e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8657e-02,  2.3742e-02,  1.9089e-02],\n",
      "          [-2.7635e-02, -1.2024e-01,  8.0003e-02],\n",
      "          [-1.2063e-01, -5.9058e-02,  8.9977e-02]],\n",
      "\n",
      "         [[-2.6579e-02,  1.0970e-01, -5.3535e-02],\n",
      "          [-6.1480e-02, -7.3522e-04,  1.1954e-01],\n",
      "          [ 7.3215e-02,  1.0305e-01,  1.0474e-01]],\n",
      "\n",
      "         [[-5.5837e-02,  5.8940e-02, -8.8949e-02],\n",
      "          [-9.5932e-02, -1.1258e-01,  2.3972e-02],\n",
      "          [-6.0986e-02, -4.0116e-02,  4.7250e-02]],\n",
      "\n",
      "         [[ 1.9824e-02,  4.2476e-02,  4.8688e-02],\n",
      "          [ 1.3394e-01,  1.0178e-01,  1.9431e-02],\n",
      "          [ 9.5733e-02,  6.8705e-02, -5.6498e-02]],\n",
      "\n",
      "         [[-7.1150e-02, -5.3285e-02, -1.1370e-01],\n",
      "          [-1.1401e-01, -1.2749e-01,  7.1731e-02],\n",
      "          [-9.7761e-02,  6.5623e-04, -8.3430e-03]],\n",
      "\n",
      "         [[-1.1452e-01,  1.0580e-01,  1.4693e-02],\n",
      "          [ 4.0077e-02,  6.6043e-02,  7.7164e-02],\n",
      "          [-1.0126e-01, -5.2605e-02,  6.1149e-03]]],\n",
      "\n",
      "\n",
      "        [[[-6.1977e-02, -1.2631e-01,  5.0198e-02],\n",
      "          [ 1.1650e-01,  1.3589e-01, -2.5687e-02],\n",
      "          [-4.8429e-02, -9.7053e-02, -2.9903e-02]],\n",
      "\n",
      "         [[-1.2566e-01,  1.3384e-01, -9.9945e-02],\n",
      "          [ 2.1487e-02, -1.0648e-01,  3.3074e-02],\n",
      "          [-4.5170e-02,  5.0528e-02, -6.1945e-03]],\n",
      "\n",
      "         [[-3.9083e-02,  1.2780e-01,  1.0363e-04],\n",
      "          [-1.1374e-01,  1.2494e-01, -4.9027e-02],\n",
      "          [-3.5736e-03,  5.1339e-02,  3.5599e-02]],\n",
      "\n",
      "         [[-4.3325e-02, -1.3285e-01,  7.5395e-02],\n",
      "          [-8.9385e-02, -3.5541e-02, -7.3601e-03],\n",
      "          [-3.0452e-02, -1.1680e-01, -1.0340e-01]],\n",
      "\n",
      "         [[-1.7141e-02, -5.9202e-02,  8.9952e-02],\n",
      "          [-7.8611e-03, -2.8096e-03,  2.8153e-02],\n",
      "          [ 4.9127e-02,  9.6489e-03,  3.1042e-02]],\n",
      "\n",
      "         [[-4.3751e-02,  5.3816e-02, -6.5163e-02],\n",
      "          [-1.2367e-01, -1.1784e-01, -5.3212e-02],\n",
      "          [ 8.4571e-02, -3.8190e-02,  8.8154e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.4715e-02, -2.8949e-02,  1.2267e-01],\n",
      "          [ 3.5580e-02,  1.2204e-01, -3.2196e-02],\n",
      "          [ 6.6010e-02,  1.5926e-02,  1.8363e-02]],\n",
      "\n",
      "         [[-5.3017e-03,  2.7520e-02,  4.9521e-02],\n",
      "          [-9.7875e-02, -6.1177e-02, -3.9450e-04],\n",
      "          [ 9.6210e-02,  7.4579e-02, -1.2128e-01]],\n",
      "\n",
      "         [[ 4.9144e-02,  7.3300e-02,  9.6473e-02],\n",
      "          [-8.0132e-02, -1.1723e-01, -2.2558e-02],\n",
      "          [-8.6258e-02,  1.2217e-01, -2.3937e-02]],\n",
      "\n",
      "         [[ 7.5617e-02, -2.8869e-04,  1.0507e-01],\n",
      "          [-2.0744e-03,  9.8274e-02, -3.6212e-02],\n",
      "          [-2.0393e-02,  7.1638e-02,  2.8653e-02]],\n",
      "\n",
      "         [[-9.3839e-02, -1.6492e-02, -4.9003e-05],\n",
      "          [-1.3016e-02, -1.0188e-01, -3.3892e-02],\n",
      "          [ 9.1597e-02,  3.7766e-02, -1.2393e-01]],\n",
      "\n",
      "         [[ 7.4309e-02,  5.1730e-02, -4.9178e-03],\n",
      "          [ 9.7945e-02, -6.2981e-02,  6.2156e-02],\n",
      "          [ 6.0329e-02,  1.2577e-02, -4.0198e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1923e-02, -1.2018e-01,  7.5570e-02],\n",
      "          [ 8.0636e-02,  4.9938e-02, -9.5346e-02],\n",
      "          [ 1.9664e-02, -7.5266e-03,  4.7080e-02]],\n",
      "\n",
      "         [[-1.1801e-01, -3.8663e-02,  1.0208e-01],\n",
      "          [-5.5286e-02,  4.3752e-02, -9.3511e-02],\n",
      "          [-4.2318e-02,  1.3507e-01, -5.0991e-02]],\n",
      "\n",
      "         [[-1.6810e-02,  6.0947e-02, -9.6693e-03],\n",
      "          [-4.4114e-02,  9.0280e-03,  3.2782e-03],\n",
      "          [ 1.0635e-01, -3.2634e-02,  3.7106e-02]],\n",
      "\n",
      "         [[ 8.0215e-02,  9.6142e-02, -1.3612e-01],\n",
      "          [-1.9136e-02, -1.1154e-01, -1.3571e-01],\n",
      "          [-9.3853e-02, -9.4059e-02, -6.3825e-02]],\n",
      "\n",
      "         [[ 6.8717e-04, -1.0797e-01, -7.2711e-02],\n",
      "          [ 7.8969e-02, -1.1641e-01,  4.9053e-02],\n",
      "          [ 5.2744e-02,  7.7068e-02, -8.5244e-02]],\n",
      "\n",
      "         [[ 7.2381e-02, -1.0690e-01, -3.3515e-02],\n",
      "          [-2.5741e-02, -8.3637e-03, -1.0960e-01],\n",
      "          [-9.9303e-02,  1.1963e-01, -1.1309e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.9446e-02,  1.2311e-01,  1.2860e-01],\n",
      "          [ 8.6964e-02, -2.6801e-02,  1.2614e-01],\n",
      "          [-1.0488e-01,  1.2729e-01,  7.1330e-02]],\n",
      "\n",
      "         [[-1.0233e-02,  1.9615e-02,  1.7947e-02],\n",
      "          [ 8.6574e-02,  6.2426e-02,  6.9250e-02],\n",
      "          [-1.3153e-01,  1.3745e-02,  4.2555e-02]],\n",
      "\n",
      "         [[-1.0614e-01, -1.1202e-01, -1.1639e-01],\n",
      "          [ 1.2345e-01,  6.2682e-02,  5.8763e-02],\n",
      "          [ 6.1489e-02, -7.7518e-02,  2.5539e-02]],\n",
      "\n",
      "         [[-1.2038e-01, -5.8619e-02, -8.0916e-02],\n",
      "          [-6.6001e-02, -4.9037e-03, -1.2008e-01],\n",
      "          [ 1.0247e-01, -1.2404e-01, -1.2942e-01]],\n",
      "\n",
      "         [[ 2.1637e-02, -6.9317e-02, -1.2191e-01],\n",
      "          [-1.3247e-01, -9.5003e-02,  1.3332e-01],\n",
      "          [-1.1982e-01, -9.5911e-02, -8.7816e-02]],\n",
      "\n",
      "         [[ 1.7871e-02, -4.4136e-02,  6.8051e-03],\n",
      "          [-9.2850e-02, -5.3514e-02,  1.0812e-01],\n",
      "          [-2.3227e-02,  1.2656e-01,  6.3932e-02]]]])\n",
      "tensor([-0.0716,  0.0372,  0.0795,  0.0758, -0.0019, -0.1173, -0.0522,  0.0036,\n",
      "        -0.0592, -0.0959, -0.1057,  0.1033, -0.1075,  0.1150,  0.1044, -0.0658])\n",
      "tensor([[ 0.0378, -0.0206, -0.0369,  ..., -0.0281,  0.0335,  0.0027],\n",
      "        [ 0.0285, -0.0096, -0.0349,  ..., -0.0338,  0.0327,  0.0395],\n",
      "        [ 0.0406,  0.0344,  0.0149,  ..., -0.0212, -0.0159, -0.0173],\n",
      "        ...,\n",
      "        [ 0.0363, -0.0037,  0.0195,  ...,  0.0243,  0.0320, -0.0220],\n",
      "        [ 0.0149,  0.0317, -0.0350,  ..., -0.0325, -0.0239,  0.0143],\n",
      "        [-0.0374, -0.0344, -0.0340,  ..., -0.0082,  0.0180,  0.0042]])\n",
      "tensor([ 1.1488e-02, -1.6033e-02, -3.9589e-05,  4.5448e-05, -4.8418e-03,\n",
      "         2.7850e-02,  2.5076e-02,  4.0524e-02,  1.5550e-02,  2.6878e-02,\n",
      "        -3.3817e-02, -3.3448e-02,  3.7707e-02, -3.9112e-02,  1.1011e-02,\n",
      "        -3.8744e-02, -3.1797e-02, -3.5822e-02,  2.0778e-02, -3.9539e-02,\n",
      "         7.0794e-03,  2.8457e-02, -2.8250e-02, -1.2550e-02,  1.4067e-02,\n",
      "        -6.5934e-03, -2.2241e-02,  1.5401e-02, -3.3480e-02, -1.9284e-02,\n",
      "         1.7569e-02, -4.0224e-03,  5.4284e-03,  3.9653e-02, -2.2670e-02,\n",
      "        -3.8036e-02, -1.6269e-03, -3.3417e-02,  1.2815e-02,  6.4585e-03,\n",
      "         3.3280e-02, -3.7615e-02, -3.0821e-02, -3.6263e-02,  2.3754e-02,\n",
      "        -1.3726e-02, -2.1084e-02, -3.9140e-02,  3.7724e-03,  2.4991e-02,\n",
      "        -3.7179e-02,  7.2712e-03,  2.9420e-02,  2.4890e-02,  3.6542e-02,\n",
      "        -1.4923e-02,  3.6731e-02,  1.7115e-02, -1.4188e-02,  3.9403e-02,\n",
      "         1.2119e-02, -3.2146e-02, -3.7221e-02, -5.7059e-03,  2.6834e-02,\n",
      "        -5.0256e-03,  3.9059e-02, -3.0099e-02, -6.8346e-03, -5.1563e-03,\n",
      "        -1.7278e-02, -2.0255e-03,  3.1473e-02,  6.3350e-03,  3.3661e-02,\n",
      "         4.5171e-04, -1.8780e-02, -1.6544e-02, -4.4983e-03,  2.0223e-02,\n",
      "         3.5800e-02,  3.5580e-02,  6.7551e-03, -2.8082e-04,  1.7153e-02,\n",
      "        -1.8246e-02,  3.6131e-03,  3.0321e-02, -2.4795e-02,  2.1380e-02,\n",
      "        -3.4088e-02,  2.3521e-02, -3.9993e-02, -1.4046e-02, -2.8005e-02,\n",
      "         2.8281e-02,  4.3808e-03, -3.6871e-02, -2.5795e-02, -2.3234e-02,\n",
      "        -2.5740e-02, -3.3962e-02, -4.1243e-02,  2.4469e-03,  3.9999e-02,\n",
      "        -3.9008e-02, -2.9574e-02,  3.7755e-02, -2.8301e-02, -3.9600e-02,\n",
      "        -3.9901e-03,  2.9360e-02, -1.9318e-02, -1.6949e-02,  2.0641e-02,\n",
      "        -6.2909e-03, -3.2229e-02,  3.3563e-02, -1.0444e-02, -2.5111e-03])\n",
      "tensor([[-0.0341, -0.0068,  0.0574,  ..., -0.0211, -0.0803, -0.0456],\n",
      "        [-0.0774,  0.0904,  0.0023,  ..., -0.0174, -0.0197, -0.0120],\n",
      "        [-0.0665,  0.0903,  0.0330,  ...,  0.0610, -0.0101,  0.0516],\n",
      "        ...,\n",
      "        [ 0.0061,  0.0193,  0.0660,  ...,  0.0714, -0.0880,  0.0358],\n",
      "        [-0.0467,  0.0589,  0.0383,  ...,  0.0864, -0.0115,  0.0775],\n",
      "        [ 0.0486,  0.0495, -0.0751,  ...,  0.0029, -0.0284,  0.0840]])\n",
      "tensor([ 0.0542, -0.0561, -0.0393,  0.0819,  0.0220, -0.0061,  0.0890, -0.0028,\n",
      "         0.0300,  0.0752,  0.0532,  0.0473, -0.0160,  0.0246,  0.0786,  0.0634,\n",
      "         0.0774,  0.0503,  0.0619, -0.0130,  0.0813,  0.0738, -0.0568,  0.0462,\n",
      "        -0.0534, -0.0382,  0.0046,  0.0079, -0.0292, -0.0884,  0.0412,  0.0328,\n",
      "         0.0487,  0.0559,  0.0042,  0.0153,  0.0075,  0.0668,  0.0518, -0.0884,\n",
      "        -0.0488,  0.0674,  0.0769, -0.0603, -0.0746,  0.0350,  0.0794,  0.0333,\n",
      "        -0.0537,  0.0344,  0.0891, -0.0281,  0.0743, -0.0099,  0.0764,  0.0274,\n",
      "         0.0891, -0.0817, -0.0532,  0.0873,  0.0102, -0.0071, -0.0650,  0.0340,\n",
      "         0.0911, -0.0165, -0.0199, -0.0676, -0.0779,  0.0512, -0.0611,  0.0540,\n",
      "         0.0312,  0.0600, -0.0831, -0.0824, -0.0269, -0.0143,  0.0658,  0.0862,\n",
      "         0.0491, -0.0423,  0.0869, -0.0892])\n",
      "tensor([[-1.0850e-01, -5.9938e-02, -8.9114e-02, -4.4695e-02,  9.7047e-02,\n",
      "          1.7383e-02, -6.3792e-02,  5.7706e-02, -1.7592e-02,  1.8070e-02,\n",
      "         -4.6507e-02,  1.1476e-02, -1.0010e-01,  1.0097e-01, -3.6965e-02,\n",
      "          1.5529e-02,  9.4486e-02,  2.7887e-03,  6.1480e-03,  7.9197e-02,\n",
      "         -7.3219e-02,  9.7839e-02, -6.3296e-02, -4.6127e-02,  8.8812e-02,\n",
      "          5.3979e-02, -6.7943e-02, -2.2410e-02,  2.5369e-02, -9.0303e-02,\n",
      "         -9.1462e-03,  9.0624e-02,  6.2351e-02, -1.0071e-01, -6.3059e-02,\n",
      "          3.3023e-02,  5.7335e-03, -5.1764e-02, -1.5084e-03,  2.9888e-03,\n",
      "         -6.1855e-02, -7.8365e-02,  4.4763e-03,  3.6057e-02, -1.2251e-02,\n",
      "         -6.6804e-02, -3.0682e-02, -3.5577e-02, -6.0683e-02,  6.2802e-02,\n",
      "          5.0472e-02, -3.6628e-02, -6.1400e-02,  1.9907e-03, -8.2899e-02,\n",
      "         -2.5604e-02,  1.0298e-01,  9.6994e-02, -8.8549e-02, -1.1323e-02,\n",
      "          5.8645e-02, -5.1158e-02,  2.6975e-02, -3.2341e-02,  7.0553e-02,\n",
      "          3.2348e-02, -1.5103e-02, -2.5763e-02,  5.3635e-02, -6.6671e-02,\n",
      "         -4.2203e-02,  2.3895e-02, -9.7102e-02,  5.8470e-02, -4.4880e-02,\n",
      "         -1.0205e-01,  1.2930e-02,  9.3929e-02,  5.8043e-02, -4.2256e-02,\n",
      "         -2.1665e-02, -1.0221e-01, -1.0337e-01,  1.2333e-03],\n",
      "        [-3.4329e-02, -5.0348e-02, -1.0581e-01,  3.6771e-02,  9.8227e-02,\n",
      "          1.2633e-02,  9.7944e-02,  7.6165e-02,  9.0765e-02,  7.1238e-02,\n",
      "          7.4405e-02, -4.9692e-02, -3.6974e-03, -4.0483e-02,  7.9274e-02,\n",
      "         -1.0012e-01, -9.4539e-02, -5.7342e-03,  7.5260e-02,  7.8839e-02,\n",
      "         -5.2203e-02,  3.1731e-02, -1.0441e-01,  1.0842e-01, -7.2366e-02,\n",
      "          6.6572e-02, -3.6495e-02, -4.7456e-03,  9.8667e-02, -5.4627e-02,\n",
      "         -8.5657e-02, -6.6137e-02,  6.3292e-02, -1.0297e-01, -1.0787e-01,\n",
      "          9.6650e-02,  7.8723e-02, -4.5059e-02,  3.4987e-02,  6.7744e-02,\n",
      "          8.4625e-02,  8.1317e-03,  4.4827e-02, -9.5828e-02,  8.3118e-02,\n",
      "          5.3388e-03,  1.1903e-02, -9.8793e-02,  6.2123e-02,  5.4752e-02,\n",
      "         -5.4666e-02, -7.7850e-02, -9.4635e-02,  7.2059e-02, -7.9515e-02,\n",
      "         -3.8999e-03,  4.2385e-02,  2.6601e-02, -1.0281e-01, -2.3225e-02,\n",
      "          9.2800e-02, -2.0263e-02, -7.7045e-02, -4.5717e-02, -8.3001e-02,\n",
      "         -2.0148e-02,  6.5383e-02, -2.5613e-02, -4.9738e-02,  8.8642e-02,\n",
      "          2.0978e-02, -5.9189e-02, -3.3510e-02,  9.5274e-02, -7.0636e-02,\n",
      "         -6.1383e-02,  7.8255e-02, -8.8804e-02,  7.4157e-02,  2.1909e-02,\n",
      "          8.0678e-02, -3.7338e-02, -5.7415e-02, -6.2900e-02],\n",
      "        [ 4.8052e-02, -1.3688e-03,  3.8958e-02,  6.2321e-02,  8.3000e-02,\n",
      "         -7.1365e-02, -7.2782e-02, -5.9556e-02,  9.7788e-02, -1.5322e-02,\n",
      "         -2.1247e-02,  8.1592e-02,  7.0524e-03, -7.7331e-02,  8.1828e-02,\n",
      "         -4.3999e-02,  2.2822e-02,  5.9414e-02,  4.8253e-02, -6.1336e-02,\n",
      "          2.2273e-02,  1.8091e-02,  6.1015e-02,  1.4287e-02,  9.2223e-02,\n",
      "         -5.6373e-02, -5.5802e-02, -6.1155e-02, -6.6640e-02,  9.0557e-02,\n",
      "          6.2775e-02, -7.5974e-02,  9.2456e-02, -4.9751e-02, -2.7501e-02,\n",
      "         -1.4542e-02,  4.6326e-02, -1.9356e-02, -5.8920e-02, -5.4171e-02,\n",
      "          5.8569e-02, -8.2906e-02,  5.6522e-02, -4.6908e-02,  4.6799e-02,\n",
      "         -8.5046e-02,  4.5882e-03, -8.7063e-02, -1.5498e-02,  8.7333e-02,\n",
      "          8.9340e-02,  3.4542e-02, -9.3242e-02,  5.4342e-02,  6.0363e-02,\n",
      "         -1.2460e-02, -7.3871e-02, -1.9886e-02,  3.5934e-02,  3.8452e-02,\n",
      "          9.5574e-03,  3.6284e-02,  1.5068e-02, -7.4795e-02, -4.0455e-03,\n",
      "          6.2413e-02,  1.0806e-01, -8.7573e-03,  9.1500e-02,  4.0935e-02,\n",
      "         -4.9670e-02,  1.7376e-02, -3.3807e-03, -6.7534e-02,  1.3899e-02,\n",
      "          7.1301e-02,  5.8554e-03, -3.7444e-02,  1.0260e-01,  4.0693e-02,\n",
      "          9.8672e-02, -5.7789e-02,  5.3518e-02, -7.0449e-02],\n",
      "        [-9.7642e-02,  6.2473e-02,  7.9559e-03,  3.3948e-02,  6.1943e-02,\n",
      "         -2.0824e-02, -3.4350e-02, -2.4306e-02, -6.8722e-03, -1.0513e-01,\n",
      "          4.0268e-02, -7.7587e-02,  1.2667e-02,  7.4366e-02,  4.2335e-02,\n",
      "          6.0230e-02, -1.9794e-02,  8.8648e-02, -9.6400e-02, -6.4346e-02,\n",
      "          8.7129e-02,  8.5483e-02,  7.8280e-03,  6.0605e-02, -8.9305e-02,\n",
      "          2.9377e-02, -3.3007e-02,  1.0684e-01,  4.3430e-02, -9.2579e-02,\n",
      "          6.3004e-02, -6.3531e-02, -5.9794e-02,  1.6491e-02, -7.9795e-03,\n",
      "         -9.4321e-03, -1.1570e-02,  5.9006e-02, -5.1125e-02, -1.9716e-02,\n",
      "         -1.3492e-02,  3.3594e-03, -5.3168e-03,  5.7240e-02,  9.3949e-02,\n",
      "         -6.3432e-02, -6.6154e-02,  6.8535e-02,  8.2648e-02, -1.0952e-01,\n",
      "         -7.1018e-02, -5.0011e-02,  4.7808e-02, -7.5635e-02, -7.9239e-02,\n",
      "         -4.4899e-02,  4.3521e-02, -6.2227e-02, -2.6817e-03,  5.2179e-03,\n",
      "          2.3635e-02,  4.8203e-02, -3.9922e-02, -4.6133e-02, -8.9138e-02,\n",
      "          7.1121e-04,  3.2049e-02,  1.0224e-01,  3.9325e-02, -3.2118e-02,\n",
      "         -1.3339e-02,  9.7994e-02,  5.3168e-04,  1.0227e-01, -1.8796e-02,\n",
      "         -2.9441e-02, -1.0560e-01, -3.1998e-02, -8.1606e-02,  5.3833e-02,\n",
      "          7.9917e-02,  8.2277e-02, -1.0490e-01,  9.8022e-02],\n",
      "        [-6.6156e-02,  8.8636e-02,  6.2556e-02, -4.4806e-02,  8.3852e-02,\n",
      "         -5.4899e-02, -5.7262e-02,  6.6818e-02, -9.5261e-02,  4.2016e-02,\n",
      "          6.8894e-02, -9.9933e-02, -6.0155e-03,  6.8347e-02, -1.0004e-01,\n",
      "         -1.3774e-03,  5.3831e-02, -8.0548e-02,  5.0312e-02,  4.5226e-02,\n",
      "          1.0845e-01,  4.3325e-02,  9.5534e-02, -1.0851e-01,  4.1598e-02,\n",
      "         -4.5111e-02,  3.0988e-02,  1.4140e-02,  4.8637e-02,  3.7261e-02,\n",
      "         -4.1307e-02,  7.2641e-02,  7.7073e-02,  9.5616e-02,  5.8829e-02,\n",
      "          6.1253e-02,  8.5267e-02,  5.9262e-03,  1.0703e-01,  6.5096e-02,\n",
      "          1.0829e-01, -7.0355e-02,  4.6640e-03,  6.8129e-02,  4.5392e-02,\n",
      "          3.4698e-02, -8.0560e-02, -8.2352e-02,  8.6300e-02,  1.0672e-01,\n",
      "          1.6452e-02,  1.9569e-02,  2.0881e-03, -1.8681e-02, -2.2896e-02,\n",
      "         -4.6064e-02, -4.6026e-02,  4.3702e-02, -6.9931e-02,  9.8804e-03,\n",
      "         -8.0981e-02,  7.9253e-02, -8.1900e-02,  5.9746e-02, -9.4384e-02,\n",
      "          2.6120e-03,  6.3423e-02, -2.2462e-02, -4.8958e-02,  2.1410e-03,\n",
      "         -2.1904e-02, -8.8849e-02, -5.4319e-02,  8.0317e-02,  9.9376e-02,\n",
      "         -7.5884e-02,  9.8152e-02, -6.5275e-02,  9.2811e-02, -6.1797e-02,\n",
      "         -8.3406e-02,  7.8741e-02,  5.4285e-02, -1.9590e-03],\n",
      "        [ 8.0306e-02,  4.1658e-02,  2.7468e-02, -2.9529e-02,  1.1501e-02,\n",
      "         -3.9859e-02, -1.0438e-01, -1.3355e-02, -4.9163e-02,  4.3918e-02,\n",
      "         -1.5545e-02,  7.6857e-03, -7.6240e-02,  2.3356e-02, -5.7524e-02,\n",
      "          2.3548e-02, -7.9917e-02, -7.0566e-02,  5.0967e-02,  6.6016e-02,\n",
      "          7.7360e-02, -5.8827e-02, -3.5612e-02,  5.3457e-02,  8.8475e-02,\n",
      "         -7.1549e-02, -7.1085e-02, -5.5736e-02, -1.3114e-02, -6.3261e-02,\n",
      "          6.3140e-02,  2.2844e-02, -9.1170e-02,  5.0744e-02, -7.2766e-02,\n",
      "         -5.2568e-02,  1.3834e-02,  4.7968e-02,  1.0710e-01,  2.1292e-02,\n",
      "          5.8726e-02, -1.0820e-01,  3.0351e-02, -6.1603e-02,  9.1539e-02,\n",
      "          8.2115e-02, -1.9904e-02, -6.9948e-02,  2.2926e-02,  6.2100e-02,\n",
      "         -5.8043e-02, -3.4830e-02,  3.7235e-02, -2.6639e-02,  2.6637e-02,\n",
      "          3.1881e-02, -5.6466e-02, -8.4739e-02,  4.0739e-02, -2.6356e-02,\n",
      "         -1.6285e-02,  4.7844e-02, -3.8121e-02, -8.1630e-02,  2.8172e-03,\n",
      "         -3.9238e-03,  4.9128e-02, -7.1621e-02, -2.8321e-02,  2.3845e-02,\n",
      "         -1.1574e-03,  4.9896e-02,  4.7860e-02, -2.1390e-02, -2.0380e-02,\n",
      "         -5.0539e-02,  6.2169e-02, -6.1109e-02,  2.0266e-06, -4.2855e-02,\n",
      "         -6.9017e-02, -4.0351e-02, -8.9625e-02, -4.0881e-02],\n",
      "        [-3.4398e-02,  1.4966e-02,  7.9735e-02, -3.8352e-02, -2.3331e-02,\n",
      "          6.8546e-02, -7.2861e-02,  9.1182e-02,  3.3752e-02,  4.4008e-02,\n",
      "         -1.3350e-02, -1.0492e-02, -7.9708e-02, -2.2134e-02,  1.3332e-02,\n",
      "          9.5860e-02, -6.8917e-02,  8.2273e-02,  9.9162e-02, -3.9507e-03,\n",
      "          3.7801e-03,  1.0026e-01,  5.5041e-02, -5.3378e-02, -2.1424e-02,\n",
      "         -6.4912e-02,  1.0454e-01, -8.6797e-02, -5.5545e-02,  6.1482e-02,\n",
      "          2.9844e-02,  1.0066e-01, -2.3715e-02,  6.8332e-02, -8.4428e-02,\n",
      "          3.6796e-02,  1.0713e-01,  9.6524e-02, -6.1953e-02, -2.8458e-02,\n",
      "          7.7740e-02,  9.7485e-02,  4.1562e-02,  5.5028e-02,  2.1147e-02,\n",
      "          1.7425e-02, -1.2043e-02,  6.6337e-02,  1.0832e-01, -9.0359e-02,\n",
      "          5.8249e-02,  6.7312e-02,  4.2300e-02, -6.1389e-03, -3.9151e-02,\n",
      "          5.6635e-02,  6.3650e-02,  2.9554e-02,  9.4867e-02,  6.4270e-02,\n",
      "          3.5041e-02,  8.7807e-02,  7.2117e-02,  4.5117e-02,  6.3855e-02,\n",
      "         -9.2547e-03,  6.8640e-02,  1.6170e-02, -1.4984e-02,  8.1292e-02,\n",
      "          7.9513e-02,  4.3001e-04,  2.9556e-02, -4.7320e-02,  8.8842e-03,\n",
      "          9.9850e-02, -1.9902e-02, -5.7804e-02,  1.0696e-01,  2.9958e-02,\n",
      "          9.9520e-02,  3.3915e-02, -4.3663e-02, -1.0299e-01],\n",
      "        [ 7.6270e-02, -6.2348e-02,  8.4251e-02, -9.5384e-03, -8.5797e-02,\n",
      "          1.0577e-01,  8.1713e-02,  3.5166e-03,  3.0580e-02,  7.2151e-02,\n",
      "         -1.3441e-02,  8.3193e-02, -9.3920e-02, -3.6280e-02,  5.8838e-02,\n",
      "         -6.7603e-02, -7.0714e-02,  2.9057e-02, -7.6236e-02,  5.4539e-02,\n",
      "         -3.8938e-02, -2.3486e-02, -1.4360e-02,  3.1967e-02, -5.9816e-02,\n",
      "         -5.1846e-02, -7.6875e-02, -1.0746e-01,  8.4498e-02,  7.2903e-02,\n",
      "         -7.4862e-02, -2.1577e-02, -8.2453e-02, -7.6986e-02,  8.5932e-02,\n",
      "          3.5151e-02,  4.8635e-02,  8.2369e-02,  5.0464e-03, -1.9797e-02,\n",
      "         -1.1430e-02,  9.7135e-02, -1.0050e-01,  5.9592e-02, -2.4929e-02,\n",
      "          5.0437e-03, -6.9112e-03, -3.3983e-02,  5.9861e-02,  5.5205e-02,\n",
      "          9.3910e-02, -4.0230e-02,  5.4060e-02, -5.3814e-02, -1.0865e-01,\n",
      "         -2.5014e-02, -1.7189e-02,  2.1051e-02, -8.5018e-02,  6.8202e-02,\n",
      "         -2.0317e-02,  2.2284e-02,  8.4857e-02,  1.6230e-02, -1.3158e-02,\n",
      "         -8.3462e-02, -3.1042e-02,  1.9395e-02, -4.6536e-02, -4.3034e-03,\n",
      "          3.7241e-02,  7.2866e-02,  9.8617e-03,  5.8526e-02,  7.4642e-02,\n",
      "         -1.0837e-01, -4.2055e-02, -7.0144e-03, -6.4806e-02, -2.2608e-02,\n",
      "          4.3571e-02,  8.4037e-02, -7.6129e-02,  2.8183e-02],\n",
      "        [-9.6254e-02, -7.2052e-02,  5.0698e-02,  1.0547e-01, -4.7424e-02,\n",
      "          7.1584e-02, -2.7483e-02, -1.5109e-02, -2.3298e-02,  1.0747e-01,\n",
      "          2.3837e-02, -1.4856e-02, -8.7146e-02, -4.0239e-02,  9.3449e-02,\n",
      "          8.3167e-02, -6.6379e-02,  1.8189e-02,  7.5264e-02, -6.1115e-02,\n",
      "         -6.8843e-02,  4.2640e-02, -6.2312e-02, -9.4960e-02, -5.0434e-02,\n",
      "          9.5080e-02, -9.0948e-02,  2.7746e-03,  6.1863e-03,  4.0293e-03,\n",
      "         -3.2066e-02,  9.1555e-02,  5.3992e-02,  1.0008e-01,  3.1737e-02,\n",
      "          5.5655e-02, -2.4281e-02,  9.5969e-02, -2.8188e-02, -1.3173e-02,\n",
      "         -6.2766e-02, -6.7562e-02, -1.7666e-02, -2.4373e-02,  4.9750e-02,\n",
      "          1.3755e-02,  1.0664e-03,  4.4660e-02,  1.8976e-02,  1.0435e-01,\n",
      "         -4.6119e-02, -4.5336e-02, -9.7031e-02, -7.3909e-02, -1.3425e-03,\n",
      "          4.1867e-02, -2.6200e-02, -1.8519e-02, -2.8367e-02,  3.4729e-02,\n",
      "          9.0798e-02,  9.7949e-02,  5.1179e-02,  8.3960e-02, -7.4385e-02,\n",
      "         -9.7629e-03,  4.1371e-02, -2.5546e-02,  9.5273e-02,  1.0853e-01,\n",
      "          9.0003e-02, -9.1637e-04,  2.0690e-02, -5.2751e-03,  5.1985e-03,\n",
      "         -8.9654e-02,  6.9715e-02,  7.8444e-02,  8.8089e-02, -5.8234e-03,\n",
      "         -3.8959e-02,  7.6965e-02,  3.1447e-02,  2.7617e-02],\n",
      "        [-9.1403e-02,  7.7235e-02, -7.0522e-02, -2.7251e-02,  7.8060e-02,\n",
      "          8.9044e-02,  7.4178e-02, -4.7538e-03, -7.8883e-02,  8.2915e-02,\n",
      "         -1.8813e-02,  5.5760e-02,  6.7269e-04,  1.8246e-02, -6.1310e-02,\n",
      "         -4.2446e-02, -1.3851e-02, -4.4394e-02,  3.5689e-02,  1.0607e-01,\n",
      "         -1.1828e-02,  7.1534e-02,  1.0013e-01, -2.0744e-02, -9.1626e-03,\n",
      "         -4.0886e-02,  1.0427e-01,  8.9966e-02, -1.5226e-03, -3.5765e-02,\n",
      "         -6.5650e-02,  4.5951e-02, -9.2102e-02,  8.9434e-02,  9.5727e-03,\n",
      "         -4.3288e-02, -8.5944e-02, -2.2088e-02,  8.7614e-03, -6.7389e-02,\n",
      "          8.8468e-02, -4.9424e-02, -2.6886e-02,  6.1052e-03, -6.5690e-02,\n",
      "         -1.4711e-02, -3.5168e-02, -1.0017e-01, -6.0437e-02,  5.1340e-02,\n",
      "         -9.7164e-02,  5.4241e-02, -1.2171e-02, -2.1549e-02,  3.1640e-02,\n",
      "          8.4035e-02,  2.5056e-02, -3.1440e-02,  3.3097e-02, -9.7966e-02,\n",
      "          3.2368e-02, -3.9797e-02, -4.1906e-02, -5.3035e-02, -8.2740e-02,\n",
      "         -7.2416e-02,  6.3050e-02,  2.2765e-02,  5.4873e-02, -2.5294e-02,\n",
      "         -1.7925e-03,  7.3495e-02,  2.4179e-02, -8.2620e-02, -9.7822e-02,\n",
      "          5.5368e-02, -9.7142e-02, -3.2751e-02, -3.2905e-02,  5.7850e-02,\n",
      "         -8.1095e-02,  1.0441e-01,  6.0801e-02, -4.2958e-02]])\n",
      "tensor([-0.0163, -0.0029, -0.0981, -0.0238,  0.0658,  0.0855,  0.0683,  0.0887,\n",
      "        -0.0550,  0.0583])\n"
     ]
    }
   ],
   "source": [
    "for f in net.parameters():\n",
    "    print(f.data.sub(f.grad.data * 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEnd an input through the model\n",
    "Calculate the ypred\n",
    "compute the loss\n",
    "get the gradient of loss with respect to all weights\n",
    "update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the learning task\n",
    "    - the appropriate sample/subject and time id\n",
    "    - y[t+1,i] = F(X[t])\n",
    "        - i = patient\n",
    "        - t = a week\n",
    "    - response variable / label\n",
    "    - what is X\n",
    "    - eval metrics\n",
    "        - loss function\n",
    "    - what is the splitting strategy?\n",
    "- define a network\n",
    "    - layers\n",
    "    - forward pass\n",
    "- send input through the network\n",
    "- calculate loss vs target\n",
    "- optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
