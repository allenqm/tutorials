{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be implementing the famous Transformer architecture from scratch.\n",
    "\n",
    "The code is based off of the following repos/blog posts:\n",
    "\n",
    "- [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) \n",
    "\n",
    "Thanks so much to their authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the keys to understanding how any model works is understanding how the shapes of the tensors change during the processing of each part. We'll be using the logging module to output debugging information to help our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "# if you want the model to continuously print tensor shapes, set to DEBUG!\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=f\"[{cls.__name__}] {name} size={tsr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use logging levels to control the modules we receive output from. The lower the logging level, the more tensor information you'll get. Feel free to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using an enum to refer to dimensions whenever possible to improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is an attention-based architecture. The attention used in the Transformer is the scaled dot product attention, represented by the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/scaled_dot_product_attention.png?zoom=2&w=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention # Logging level: \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # compute the dot product between queries and keys for\n",
    "        # each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
    "        # we get an attention score between each position in the sequence\n",
    "        # for each batch\n",
    "\n",
    "        # scale the dot products by the dimensionality (see the paper for why we do this!)\n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        # normalize the weights across the sequence dimension\n",
    "        # (Note that since we transposed, the sequence and feature dimensions are switched)\n",
    "        attn = torch.exp(attn)\n",
    "        log_size(attn, \"attention weight\") # (Batch, Seq, Seq)\n",
    "        \n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5621, 0.4720, 0.5329, 0.4297, 0.5339, 0.4660, 0.7556, 0.6481,\n",
       "          0.5159, 0.5285, 0.3473, 0.5580, 0.5791, 0.6454, 0.4410, 0.6190,\n",
       "          0.5605, 0.6490, 0.5019, 0.4977],\n",
       "         [0.4839, 0.3685, 0.4248, 0.4280, 0.4808, 0.3795, 0.7199, 0.5827,\n",
       "          0.4373, 0.4643, 0.2958, 0.3993, 0.4992, 0.6386, 0.4102, 0.4555,\n",
       "          0.4649, 0.5748, 0.4454, 0.4761],\n",
       "         [0.5586, 0.4803, 0.5412, 0.4233, 0.5409, 0.4835, 0.7533, 0.6576,\n",
       "          0.5251, 0.5308, 0.3540, 0.5542, 0.5887, 0.6446, 0.4336, 0.6424,\n",
       "          0.5745, 0.6643, 0.5125, 0.5048],\n",
       "         [0.5396, 0.4681, 0.5228, 0.4202, 0.5556, 0.4575, 0.7571, 0.6587,\n",
       "          0.5126, 0.5396, 0.3577, 0.5636, 0.5809, 0.6459, 0.4377, 0.6174,\n",
       "          0.5579, 0.6536, 0.4923, 0.4734],\n",
       "         [0.4879, 0.4757, 0.5197, 0.3979, 0.4973, 0.4596, 0.6580, 0.6172,\n",
       "          0.4789, 0.5292, 0.3193, 0.5004, 0.5664, 0.5976, 0.3650, 0.6486,\n",
       "          0.5525, 0.6514, 0.4817, 0.4642],\n",
       "         [0.5560, 0.4693, 0.5312, 0.4282, 0.5327, 0.4654, 0.7504, 0.6498,\n",
       "          0.5097, 0.5219, 0.3572, 0.5542, 0.5888, 0.6375, 0.4217, 0.6277,\n",
       "          0.5709, 0.6695, 0.4984, 0.5016],\n",
       "         [0.4898, 0.3888, 0.4458, 0.4119, 0.5380, 0.4458, 0.7517, 0.6491,\n",
       "          0.5039, 0.4824, 0.3445, 0.4581, 0.5310, 0.6368, 0.4273, 0.5479,\n",
       "          0.5319, 0.6176, 0.5052, 0.5037],\n",
       "         [0.4684, 0.3986, 0.4447, 0.3692, 0.4860, 0.3626, 0.6635, 0.6095,\n",
       "          0.4467, 0.4773, 0.3301, 0.5696, 0.4705, 0.5597, 0.3944, 0.5194,\n",
       "          0.4658, 0.5559, 0.4045, 0.3678],\n",
       "         [0.5744, 0.4704, 0.5308, 0.4426, 0.5364, 0.4613, 0.7645, 0.6371,\n",
       "          0.5122, 0.5076, 0.3504, 0.5404, 0.5979, 0.6448, 0.4321, 0.6195,\n",
       "          0.5713, 0.6747, 0.5140, 0.5308],\n",
       "         [0.5501, 0.4713, 0.5225, 0.4177, 0.5593, 0.4566, 0.7548, 0.6501,\n",
       "          0.5096, 0.5231, 0.3572, 0.5602, 0.5851, 0.6337, 0.4296, 0.6262,\n",
       "          0.5596, 0.6676, 0.4905, 0.4827]],\n",
       "\n",
       "        [[0.4031, 0.3594, 0.2133, 0.5058, 0.3426, 0.5334, 0.3807, 0.3425,\n",
       "          0.3811, 0.3050, 0.3367, 0.5140, 0.2360, 0.4389, 0.5168, 0.5246,\n",
       "          0.5116, 0.4151, 0.5566, 0.5341],\n",
       "         [0.4735, 0.6450, 0.3473, 0.5653, 0.2633, 0.6872, 0.5776, 0.4247,\n",
       "          0.5203, 0.4126, 0.4586, 0.6047, 0.3493, 0.4784, 0.6976, 0.5609,\n",
       "          0.5103, 0.4847, 0.6434, 0.5551],\n",
       "         [0.5253, 0.5446, 0.3428, 0.6250, 0.4056, 0.6697, 0.5784, 0.4933,\n",
       "          0.4896, 0.4839, 0.4247, 0.6458, 0.3294, 0.5338, 0.6797, 0.6717,\n",
       "          0.6053, 0.4869, 0.6894, 0.5793],\n",
       "         [0.4454, 0.5520, 0.2693, 0.5070, 0.2778, 0.5868, 0.5821, 0.4730,\n",
       "          0.4752, 0.3568, 0.2640, 0.5135, 0.3343, 0.5273, 0.6389, 0.5512,\n",
       "          0.4254, 0.4396, 0.5915, 0.4785],\n",
       "         [0.5541, 0.6717, 0.3607, 0.6293, 0.3661, 0.7230, 0.6530, 0.5272,\n",
       "          0.5575, 0.4857, 0.4497, 0.6826, 0.3543, 0.5834, 0.7814, 0.6628,\n",
       "          0.5788, 0.5491, 0.7275, 0.6191],\n",
       "         [0.4294, 0.5483, 0.2993, 0.5685, 0.3522, 0.6920, 0.5415, 0.4346,\n",
       "          0.5506, 0.3943, 0.3529, 0.6051, 0.3510, 0.5963, 0.6622, 0.6416,\n",
       "          0.5715, 0.4931, 0.6394, 0.5975],\n",
       "         [0.5256, 0.5692, 0.3521, 0.5660, 0.3701, 0.6570, 0.6209, 0.5215,\n",
       "          0.4721, 0.4631, 0.3540, 0.6669, 0.3000, 0.4973, 0.6760, 0.6078,\n",
       "          0.5745, 0.4832, 0.6727, 0.6081],\n",
       "         [0.4740, 0.6592, 0.3425, 0.5649, 0.2582, 0.6852, 0.5850, 0.4260,\n",
       "          0.5233, 0.4068, 0.4589, 0.5988, 0.3474, 0.4839, 0.7141, 0.5556,\n",
       "          0.4948, 0.4908, 0.6394, 0.5490],\n",
       "         [0.5195, 0.5614, 0.3395, 0.5604, 0.3608, 0.6334, 0.6126, 0.5040,\n",
       "          0.4644, 0.4465, 0.3493, 0.6430, 0.2826, 0.4841, 0.6749, 0.5746,\n",
       "          0.5503, 0.4843, 0.6604, 0.5967],\n",
       "         [0.4209, 0.4484, 0.2347, 0.5805, 0.3083, 0.5828, 0.5521, 0.4804,\n",
       "          0.4452, 0.3838, 0.3549, 0.4525, 0.2966, 0.4457, 0.5846, 0.5069,\n",
       "          0.4332, 0.4599, 0.5942, 0.4282]],\n",
       "\n",
       "        [[0.4546, 0.3209, 0.5072, 0.5322, 0.3857, 0.4168, 0.7539, 0.4663,\n",
       "          0.5859, 0.3660, 0.6428, 0.6259, 0.5950, 0.4891, 0.5142, 0.5588,\n",
       "          0.7192, 0.3720, 0.5057, 0.4077],\n",
       "         [0.4888, 0.3425, 0.5226, 0.6110, 0.4607, 0.5260, 0.8044, 0.5325,\n",
       "          0.6426, 0.4586, 0.7551, 0.7083, 0.6007, 0.5262, 0.5317, 0.6288,\n",
       "          0.7551, 0.4363, 0.5074, 0.5168],\n",
       "         [0.3775, 0.3021, 0.4235, 0.4570, 0.4038, 0.4253, 0.6067, 0.5022,\n",
       "          0.5737, 0.3728, 0.5470, 0.5469, 0.4124, 0.4431, 0.4974, 0.4628,\n",
       "          0.6665, 0.3144, 0.4647, 0.4676],\n",
       "         [0.4631, 0.2696, 0.4831, 0.5993, 0.4504, 0.5056, 0.7451, 0.4852,\n",
       "          0.5881, 0.4293, 0.7180, 0.6635, 0.5520, 0.5030, 0.4755, 0.5547,\n",
       "          0.6933, 0.3519, 0.4334, 0.4247],\n",
       "         [0.4463, 0.3046, 0.5118, 0.5340, 0.3802, 0.4266, 0.7419, 0.4524,\n",
       "          0.5591, 0.3637, 0.6444, 0.6226, 0.5772, 0.4739, 0.4801, 0.5428,\n",
       "          0.7161, 0.3707, 0.4972, 0.3961],\n",
       "         [0.4888, 0.3470, 0.5352, 0.6114, 0.4658, 0.5223, 0.8047, 0.5371,\n",
       "          0.6418, 0.4616, 0.7504, 0.7072, 0.5983, 0.5307, 0.5398, 0.6204,\n",
       "          0.7764, 0.4315, 0.5193, 0.5087],\n",
       "         [0.4580, 0.2706, 0.4868, 0.6057, 0.4422, 0.5022, 0.7447, 0.4818,\n",
       "          0.5829, 0.4341, 0.7223, 0.6555, 0.5453, 0.5026, 0.4810, 0.5501,\n",
       "          0.6936, 0.3615, 0.4387, 0.4292],\n",
       "         [0.4254, 0.3064, 0.4610, 0.5489, 0.4403, 0.4987, 0.7402, 0.4763,\n",
       "          0.5890, 0.4456, 0.7002, 0.6667, 0.5881, 0.5039, 0.5049, 0.6063,\n",
       "          0.6992, 0.4153, 0.4542, 0.5057],\n",
       "         [0.4927, 0.3827, 0.5345, 0.6035, 0.4568, 0.5099, 0.8150, 0.5398,\n",
       "          0.6365, 0.4696, 0.7523, 0.7032, 0.5994, 0.5221, 0.5459, 0.6411,\n",
       "          0.7648, 0.4566, 0.5265, 0.5116],\n",
       "         [0.4257, 0.3003, 0.4659, 0.5576, 0.4515, 0.5189, 0.7443, 0.4894,\n",
       "          0.5853, 0.4480, 0.7099, 0.6738, 0.5759, 0.5040, 0.4952, 0.6050,\n",
       "          0.7106, 0.4097, 0.4579, 0.5080]],\n",
       "\n",
       "        [[0.5179, 0.4544, 0.5645, 0.6166, 0.4514, 0.6402, 0.6386, 0.5160,\n",
       "          0.4912, 0.5422, 0.5869, 0.3568, 0.4090, 0.5184, 0.6865, 0.4616,\n",
       "          0.6072, 0.3410, 0.4770, 0.5813],\n",
       "         [0.4506, 0.3525, 0.2852, 0.5280, 0.3837, 0.4774, 0.3706, 0.3209,\n",
       "          0.3236, 0.3557, 0.4326, 0.2842, 0.3057, 0.4065, 0.4562, 0.3402,\n",
       "          0.4738, 0.2008, 0.3897, 0.4080],\n",
       "         [0.5046, 0.4553, 0.5675, 0.6236, 0.4620, 0.6539, 0.6546, 0.5225,\n",
       "          0.4988, 0.5457, 0.5803, 0.3626, 0.4037, 0.5060, 0.6765, 0.4721,\n",
       "          0.5936, 0.3486, 0.4874, 0.5876],\n",
       "         [0.5177, 0.4662, 0.5580, 0.6302, 0.4716, 0.6404, 0.6456, 0.5195,\n",
       "          0.5093, 0.5403, 0.5782, 0.3631, 0.4062, 0.5140, 0.6769, 0.4674,\n",
       "          0.5912, 0.3310, 0.4861, 0.5679],\n",
       "         [0.4986, 0.4423, 0.5703, 0.6222, 0.4443, 0.6429, 0.6628, 0.5084,\n",
       "          0.5028, 0.5635, 0.5837, 0.3676, 0.4299, 0.5345, 0.6933, 0.4651,\n",
       "          0.6143, 0.3400, 0.4669, 0.5783],\n",
       "         [0.4958, 0.4341, 0.5720, 0.6188, 0.4366, 0.6532, 0.6647, 0.5078,\n",
       "          0.4935, 0.5692, 0.5896, 0.3706, 0.4299, 0.5336, 0.6958, 0.4651,\n",
       "          0.6200, 0.3490, 0.4634, 0.5916],\n",
       "         [0.5121, 0.4553, 0.5696, 0.6232, 0.4700, 0.6430, 0.6477, 0.5137,\n",
       "          0.5106, 0.5461, 0.5790, 0.3568, 0.4077, 0.5214, 0.6811, 0.4673,\n",
       "          0.5970, 0.3369, 0.4773, 0.5728],\n",
       "         [0.5013, 0.4415, 0.5745, 0.6143, 0.4472, 0.6558, 0.6558, 0.5142,\n",
       "          0.4921, 0.5571, 0.5878, 0.3577, 0.4159, 0.5199, 0.6902, 0.4729,\n",
       "          0.6118, 0.3523, 0.4734, 0.5953],\n",
       "         [0.4802, 0.3895, 0.3987, 0.5467, 0.3886, 0.5859, 0.4997, 0.4301,\n",
       "          0.3746, 0.4623, 0.5342, 0.3116, 0.3400, 0.4236, 0.5510, 0.4170,\n",
       "          0.5271, 0.2839, 0.4159, 0.5034],\n",
       "         [0.3510, 0.2912, 0.4914, 0.5044, 0.4153, 0.6145, 0.5901, 0.3693,\n",
       "          0.4573, 0.5323, 0.4719, 0.2905, 0.3570, 0.4594, 0.5585, 0.4384,\n",
       "          0.4924, 0.2947, 0.3306, 0.5013]],\n",
       "\n",
       "        [[0.4032, 0.5512, 0.6259, 0.5145, 0.3974, 0.5704, 0.6057, 0.7734,\n",
       "          0.4136, 0.4341, 0.5914, 0.5189, 0.6395, 0.5307, 0.6611, 0.5994,\n",
       "          0.4526, 0.5445, 0.5371, 0.5497],\n",
       "         [0.3331, 0.5439, 0.4813, 0.4465, 0.3830, 0.5182, 0.5729, 0.6743,\n",
       "          0.4219, 0.4115, 0.5188, 0.4120, 0.5855, 0.4320, 0.5301, 0.5405,\n",
       "          0.4467, 0.4252, 0.4696, 0.5196],\n",
       "         [0.3785, 0.5033, 0.5866, 0.4392, 0.3335, 0.3773, 0.4679, 0.6117,\n",
       "          0.2848, 0.3550, 0.4093, 0.3870, 0.5519, 0.4852, 0.5943, 0.4625,\n",
       "          0.4274, 0.4600, 0.4354, 0.4551],\n",
       "         [0.3445, 0.4854, 0.5587, 0.4900, 0.3971, 0.5274, 0.5245, 0.6685,\n",
       "          0.3904, 0.3650, 0.4897, 0.4577, 0.5589, 0.5015, 0.5694, 0.5337,\n",
       "          0.3656, 0.5302, 0.4276, 0.5379],\n",
       "         [0.3563, 0.5111, 0.5344, 0.4176, 0.3562, 0.4136, 0.5119, 0.6025,\n",
       "          0.3367, 0.3257, 0.4908, 0.4003, 0.4771, 0.4829, 0.6028, 0.4401,\n",
       "          0.4130, 0.4291, 0.4463, 0.4503],\n",
       "         [0.3175, 0.4634, 0.5572, 0.4321, 0.3256, 0.5443, 0.5203, 0.7048,\n",
       "          0.3999, 0.3796, 0.5258, 0.4321, 0.5862, 0.4685, 0.5949, 0.5577,\n",
       "          0.4259, 0.5031, 0.4397, 0.4888],\n",
       "         [0.3096, 0.4459, 0.4372, 0.3990, 0.3522, 0.4855, 0.5072, 0.5550,\n",
       "          0.3304, 0.3927, 0.4741, 0.3869, 0.5161, 0.3304, 0.4759, 0.5198,\n",
       "          0.3494, 0.3410, 0.4881, 0.4088],\n",
       "         [0.3733, 0.5195, 0.5344, 0.4311, 0.3562, 0.5005, 0.5719, 0.7294,\n",
       "          0.4081, 0.3877, 0.5742, 0.4890, 0.5595, 0.5277, 0.6086, 0.5235,\n",
       "          0.4403, 0.4865, 0.4601, 0.5146],\n",
       "         [0.3714, 0.5247, 0.5089, 0.4257, 0.3835, 0.5281, 0.5859, 0.7225,\n",
       "          0.4125, 0.4091, 0.6013, 0.4913, 0.5767, 0.4994, 0.6141, 0.5580,\n",
       "          0.4484, 0.4633, 0.4898, 0.5052],\n",
       "         [0.4179, 0.5577, 0.6161, 0.5210, 0.3946, 0.5565, 0.6057, 0.7826,\n",
       "          0.3967, 0.4305, 0.5805, 0.5126, 0.6494, 0.5241, 0.6585, 0.5922,\n",
       "          0.4610, 0.5427, 0.5364, 0.5466]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn to the core component in the Transformer architecture: the multi-head attention block. This block applies linear transformations to the input, then applies scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/multi_head_attention.png?zoom=2&resize=224%2C293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.4944e-02,  1.7957e-01, -1.1838e-01, -4.6875e-02, -1.6914e-01,\n",
       "           1.0049e-01,  2.6786e-01, -2.1204e-01,  1.4735e-01, -1.8410e-01,\n",
       "           1.0990e-01,  9.9212e-02,  1.5214e-01, -4.1235e-01, -1.3164e-02,\n",
       "          -4.0524e-01, -1.5269e-01, -1.4364e-01, -3.1397e-01,  4.4045e-01],\n",
       "         [-5.9720e-02,  1.6309e-01, -1.0697e-01, -5.2231e-02, -1.1765e-01,\n",
       "           8.0843e-02,  2.5074e-01, -1.8122e-01,  1.5265e-01, -1.7002e-01,\n",
       "           1.1297e-01,  6.7675e-02,  1.4157e-01, -3.7354e-01, -1.3119e-02,\n",
       "          -3.4405e-01, -1.4579e-01, -1.3223e-01, -3.0427e-01,  3.9197e-01],\n",
       "         [-6.8037e-02,  1.5504e-01, -1.1897e-01, -1.8954e-02, -1.4505e-01,\n",
       "           1.0521e-01,  2.5207e-01, -2.1201e-01,  1.3030e-01, -1.5855e-01,\n",
       "           1.2356e-01,  6.4783e-02,  1.2035e-01, -3.7260e-01,  3.3556e-03,\n",
       "          -3.7950e-01, -1.2843e-01, -1.4307e-01, -2.5830e-01,  4.1091e-01],\n",
       "         [-3.1409e-02,  1.6074e-01, -9.2922e-02, -4.4190e-02, -1.7137e-01,\n",
       "           1.2676e-01,  2.2969e-01, -1.6788e-01,  1.2302e-01, -1.5437e-01,\n",
       "           9.0397e-02,  8.8295e-02,  1.7164e-01, -3.4605e-01, -2.1286e-02,\n",
       "          -3.7234e-01, -1.2822e-01, -1.4102e-01, -2.9214e-01,  4.1309e-01],\n",
       "         [-2.8923e-02,  1.0444e-01, -8.1204e-02, -1.6117e-02, -1.2152e-01,\n",
       "           1.0161e-01,  1.5600e-01, -8.7263e-02,  8.5259e-02, -1.2596e-01,\n",
       "           8.5037e-02,  5.8773e-02,  1.3754e-01, -2.6274e-01, -1.0367e-03,\n",
       "          -2.8459e-01, -9.7668e-02, -9.2387e-02, -2.0926e-01,  3.0287e-01],\n",
       "         [ 1.3661e-02,  1.2381e-01, -4.5423e-02, -3.6575e-02, -1.4307e-01,\n",
       "           8.1547e-02,  1.7382e-01, -1.5457e-01,  1.1728e-01, -9.4260e-02,\n",
       "           5.6016e-02,  8.4651e-02,  1.1691e-01, -2.4253e-01, -1.0516e-02,\n",
       "          -3.3524e-01, -9.8983e-02, -7.5423e-02, -2.2950e-01,  3.1669e-01],\n",
       "         [-3.1591e-02,  1.5897e-01, -9.0789e-02, -4.1917e-02, -1.6739e-01,\n",
       "           1.2631e-01,  2.3073e-01, -1.6560e-01,  1.2655e-01, -1.5280e-01,\n",
       "           9.1396e-02,  8.5005e-02,  1.7083e-01, -3.4450e-01, -2.0566e-02,\n",
       "          -3.6949e-01, -1.2873e-01, -1.4084e-01, -2.9374e-01,  4.0992e-01],\n",
       "         [-5.9084e-02,  1.7863e-01, -1.1228e-01, -5.7833e-02, -1.0816e-01,\n",
       "           2.7647e-02,  1.5734e-01, -1.0828e-01,  5.3929e-02, -1.5332e-01,\n",
       "           7.8254e-02,  9.3015e-02,  1.1347e-01, -3.1401e-01, -2.7824e-02,\n",
       "          -2.4565e-01, -1.1899e-01, -8.1333e-02, -2.3312e-01,  2.7805e-01],\n",
       "         [-2.5298e-02,  1.5031e-01, -7.1708e-02, -7.2793e-02, -1.0371e-01,\n",
       "           5.5523e-02,  1.9792e-01, -1.5354e-01,  1.4380e-01, -1.5556e-01,\n",
       "           9.2340e-02,  7.5291e-02,  1.2018e-01, -3.0553e-01,  3.4992e-04,\n",
       "          -3.4561e-01, -1.1472e-01, -8.3350e-02, -2.6740e-01,  3.5730e-01],\n",
       "         [-5.4182e-02,  1.8136e-01, -1.1600e-01, -4.7195e-02, -1.6755e-01,\n",
       "           9.9505e-02,  2.6882e-01, -2.1202e-01,  1.5004e-01, -1.8430e-01,\n",
       "           1.0995e-01,  9.8980e-02,  1.5194e-01, -4.1209e-01, -1.3114e-02,\n",
       "          -4.0589e-01, -1.5252e-01, -1.4318e-01, -3.1685e-01,  4.4098e-01]],\n",
       "\n",
       "        [[-9.5346e-02,  1.9056e-01, -8.4607e-02, -1.6479e-01, -3.1116e-01,\n",
       "           2.2971e-01,  3.5047e-01, -1.1053e-01,  4.9899e-02,  1.0346e-01,\n",
       "          -3.2906e-02, -8.5523e-02,  2.1613e-01, -3.7920e-01,  6.3413e-02,\n",
       "          -3.0744e-01, -1.5210e-01, -3.6353e-01, -3.5361e-01,  3.5672e-01],\n",
       "         [-1.2780e-01,  2.1504e-01, -1.3189e-01, -1.9318e-01, -3.4042e-01,\n",
       "           2.5891e-01,  3.8123e-01, -1.3523e-01,  3.2923e-02,  9.4564e-02,\n",
       "          -3.0385e-02, -1.0726e-01,  2.3878e-01, -4.4021e-01,  8.3445e-02,\n",
       "          -2.9966e-01, -1.7209e-01, -4.1656e-01, -3.8432e-01,  4.0546e-01],\n",
       "         [-1.2323e-01,  2.1594e-01, -1.2887e-01, -1.8518e-01, -3.0335e-01,\n",
       "           2.1960e-01,  3.2928e-01, -1.0163e-01,  4.9445e-03,  9.4288e-02,\n",
       "          -3.3723e-02, -8.2470e-02,  2.1394e-01, -3.7791e-01,  7.4966e-02,\n",
       "          -2.8958e-01, -1.3134e-01, -3.7821e-01, -3.3867e-01,  3.5248e-01],\n",
       "         [-1.3046e-01,  2.1486e-01, -1.3503e-01, -1.9364e-01, -3.3995e-01,\n",
       "           2.5805e-01,  3.8130e-01, -1.3612e-01,  3.1058e-02,  9.5263e-02,\n",
       "          -2.9091e-02, -1.0914e-01,  2.3936e-01, -4.4024e-01,  8.5177e-02,\n",
       "          -3.0018e-01, -1.7109e-01, -4.2144e-01, -3.8258e-01,  4.0534e-01],\n",
       "         [-1.3500e-01,  1.7647e-01, -1.4262e-01, -1.5502e-01, -3.0269e-01,\n",
       "           2.1541e-01,  2.7558e-01, -6.9251e-02, -3.3343e-02,  8.0866e-02,\n",
       "          -2.6729e-02, -7.3746e-02,  2.0292e-01, -3.4834e-01,  6.7515e-02,\n",
       "          -2.3356e-01, -1.1191e-01, -3.4817e-01, -2.8999e-01,  3.2248e-01],\n",
       "         [-1.2127e-01,  1.6372e-01, -1.3851e-01, -1.7506e-01, -3.0279e-01,\n",
       "           2.1938e-01,  3.0752e-01, -6.6194e-02, -2.0564e-02,  6.5271e-02,\n",
       "          -4.8357e-02, -8.4177e-02,  2.3847e-01, -3.7405e-01,  1.0077e-01,\n",
       "          -2.0441e-01, -1.1742e-01, -3.5977e-01, -3.0345e-01,  3.4553e-01],\n",
       "         [-1.2193e-01,  2.1052e-01, -1.2977e-01, -1.8403e-01, -3.0186e-01,\n",
       "           2.1956e-01,  3.3071e-01, -1.0277e-01,  7.8332e-03,  9.2372e-02,\n",
       "          -3.0977e-02, -8.2361e-02,  2.1137e-01, -3.7905e-01,  7.1468e-02,\n",
       "          -2.8863e-01, -1.3589e-01, -3.7932e-01, -3.3825e-01,  3.4982e-01],\n",
       "         [-9.9686e-02,  1.9752e-01, -1.0871e-01, -1.6783e-01, -3.0752e-01,\n",
       "           2.3420e-01,  3.0679e-01, -1.1224e-01,  2.6455e-02,  8.1940e-02,\n",
       "          -3.6011e-02, -8.9657e-02,  2.2772e-01, -3.6861e-01,  7.9822e-02,\n",
       "          -2.8495e-01, -1.5294e-01, -3.7006e-01, -3.4867e-01,  3.6493e-01],\n",
       "         [-1.3132e-01,  2.1881e-01, -1.3321e-01, -1.9401e-01, -3.4286e-01,\n",
       "           2.5903e-01,  3.8066e-01, -1.3355e-01,  2.9071e-02,  9.6852e-02,\n",
       "          -3.1567e-02, -1.0845e-01,  2.4115e-01, -4.4075e-01,  8.6507e-02,\n",
       "          -2.9972e-01, -1.6829e-01, -4.1734e-01, -3.8412e-01,  4.0802e-01],\n",
       "         [-1.2795e-01,  2.1591e-01, -1.3061e-01, -1.9298e-01, -3.4180e-01,\n",
       "           2.5910e-01,  3.8082e-01, -1.3430e-01,  3.2713e-02,  9.5812e-02,\n",
       "          -3.1114e-02, -1.0746e-01,  2.3985e-01, -4.3991e-01,  8.4028e-02,\n",
       "          -3.0076e-01, -1.7137e-01, -4.1600e-01, -3.8497e-01,  4.0640e-01]],\n",
       "\n",
       "        [[ 1.4829e-02,  2.1466e-01, -7.7537e-02, -1.1814e-01, -1.5669e-01,\n",
       "           7.4532e-02,  3.7048e-01, -1.1523e-01,  1.4532e-01, -9.1614e-02,\n",
       "           3.1689e-02,  8.4116e-02,  2.6219e-01, -3.9665e-01, -3.6007e-02,\n",
       "          -3.4356e-01, -1.5364e-01, -1.6909e-01, -3.2260e-01,  3.6562e-01],\n",
       "         [ 1.2222e-02,  2.1777e-01, -7.8975e-02, -1.2269e-01, -1.6029e-01,\n",
       "           7.7721e-02,  3.7200e-01, -1.1559e-01,  1.3827e-01, -8.9875e-02,\n",
       "           2.8033e-02,  8.2171e-02,  2.6525e-01, -4.0068e-01, -3.6668e-02,\n",
       "          -3.4356e-01, -1.5779e-01, -1.7740e-01, -3.2411e-01,  3.6676e-01],\n",
       "         [-5.1844e-03,  2.0500e-01, -7.9305e-02, -9.6341e-02, -1.5449e-01,\n",
       "           1.1155e-01,  3.4756e-01, -1.1345e-01,  9.8995e-02, -8.6267e-02,\n",
       "           4.2021e-02,  7.6822e-02,  2.6181e-01, -3.8881e-01, -3.9381e-02,\n",
       "          -3.3072e-01, -1.5053e-01, -1.7715e-01, -3.1080e-01,  3.5546e-01],\n",
       "         [-2.1390e-02,  2.2589e-01, -1.1375e-01, -1.3345e-01, -1.8775e-01,\n",
       "           1.1796e-01,  4.1775e-01, -1.2855e-01,  1.2348e-01, -8.4217e-02,\n",
       "           4.3230e-02,  6.2473e-02,  2.8100e-01, -4.5636e-01, -4.3536e-02,\n",
       "          -3.4906e-01, -1.8292e-01, -2.2904e-01, -3.4499e-01,  3.9972e-01],\n",
       "         [-3.6772e-02,  1.9558e-01, -1.1361e-01, -1.3155e-01, -1.7036e-01,\n",
       "           1.1338e-01,  3.6369e-01, -1.2287e-01,  5.3102e-02, -5.8838e-02,\n",
       "           3.1334e-02,  2.8845e-02,  2.8091e-01, -3.9724e-01, -5.8492e-02,\n",
       "          -2.7757e-01, -1.8039e-01, -2.4990e-01, -2.8602e-01,  3.5020e-01],\n",
       "         [-2.2908e-02,  2.2986e-01, -1.1281e-01, -1.3637e-01, -1.8735e-01,\n",
       "           1.1824e-01,  4.1561e-01, -1.3088e-01,  1.1701e-01, -7.9401e-02,\n",
       "           4.2063e-02,  5.9131e-02,  2.8316e-01, -4.5286e-01, -4.4839e-02,\n",
       "          -3.4862e-01, -1.8275e-01, -2.3177e-01, -3.4544e-01,  4.0051e-01],\n",
       "         [-2.7140e-02,  2.3290e-01, -1.0759e-01, -1.2655e-01, -1.7530e-01,\n",
       "           1.0812e-01,  3.5631e-01, -1.1207e-01,  7.0369e-02, -5.1960e-02,\n",
       "           2.8012e-02,  6.5634e-02,  2.6498e-01, -3.8771e-01, -3.1356e-02,\n",
       "          -3.3387e-01, -1.3063e-01, -1.9210e-01, -3.1233e-01,  3.7562e-01],\n",
       "         [-4.2689e-02,  1.8836e-01, -1.2387e-01, -1.1283e-01, -1.6609e-01,\n",
       "           1.0105e-01,  3.2999e-01, -9.1326e-02,  3.5324e-02, -5.5362e-02,\n",
       "           2.5807e-02,  4.9027e-02,  2.2834e-01, -3.7598e-01, -2.4894e-02,\n",
       "          -2.6879e-01, -1.3521e-01, -1.8502e-01, -2.4898e-01,  3.2533e-01],\n",
       "         [-2.3862e-02,  2.2986e-01, -1.1449e-01, -1.3495e-01, -1.8726e-01,\n",
       "           1.2010e-01,  4.1575e-01, -1.3006e-01,  1.1801e-01, -7.9141e-02,\n",
       "           4.3972e-02,  5.9895e-02,  2.8235e-01, -4.5187e-01, -4.4372e-02,\n",
       "          -3.4969e-01, -1.7930e-01, -2.2956e-01, -3.4582e-01,  4.0222e-01],\n",
       "         [ 1.1533e-02,  2.1780e-01, -7.9977e-02, -1.2020e-01, -1.5922e-01,\n",
       "           7.6380e-02,  3.7212e-01, -1.1732e-01,  1.3924e-01, -8.9382e-02,\n",
       "           3.0790e-02,  8.1687e-02,  2.6376e-01, -4.0047e-01, -3.6006e-02,\n",
       "          -3.4345e-01, -1.5646e-01, -1.7367e-01, -3.2342e-01,  3.6789e-01]],\n",
       "\n",
       "        [[ 5.2638e-04,  1.4062e-01, -3.8333e-02, -1.8413e-01, -1.4515e-01,\n",
       "           1.1048e-01,  2.9365e-01, -1.3981e-01,  5.0061e-02,  3.7893e-02,\n",
       "           3.8073e-02, -1.8346e-02,  2.3063e-01, -2.3147e-01, -4.6018e-02,\n",
       "          -3.2472e-01, -1.2134e-01, -1.7252e-01, -2.6914e-01,  3.5427e-01],\n",
       "         [-3.3543e-02,  1.7334e-01, -3.9363e-02, -2.3239e-01, -2.0141e-01,\n",
       "           1.5847e-01,  4.1443e-01, -1.6635e-01,  7.0574e-02,  4.2276e-02,\n",
       "           1.8342e-02, -3.9536e-02,  2.5631e-01, -3.2458e-01, -2.3745e-02,\n",
       "          -3.9337e-01, -1.4273e-01, -3.1120e-01, -3.2764e-01,  4.1466e-01],\n",
       "         [-3.4982e-02,  1.7314e-01, -4.0729e-02, -2.3172e-01, -2.0333e-01,\n",
       "           1.6001e-01,  4.1771e-01, -1.6859e-01,  7.2163e-02,  4.3452e-02,\n",
       "           1.8814e-02, -4.0698e-02,  2.5493e-01, -3.2703e-01, -2.3779e-02,\n",
       "          -3.9257e-01, -1.4318e-01, -3.1365e-01, -3.2869e-01,  4.1616e-01],\n",
       "         [-5.0065e-02,  1.2541e-01, -2.5776e-02, -2.0107e-01, -1.7415e-01,\n",
       "           1.5885e-01,  3.6277e-01, -1.3542e-01,  4.2996e-02,  2.7649e-02,\n",
       "           9.5956e-03, -3.9579e-02,  2.1460e-01, -2.7966e-01, -2.2125e-02,\n",
       "          -3.4423e-01, -1.2129e-01, -3.0603e-01, -2.6714e-01,  3.5181e-01],\n",
       "         [-3.4709e-02,  1.3265e-01, -3.4767e-02, -2.1808e-01, -2.0197e-01,\n",
       "           1.6698e-01,  3.7508e-01, -1.3017e-01,  5.7457e-02,  3.2557e-02,\n",
       "           1.0198e-02, -4.7651e-02,  2.4264e-01, -2.9805e-01, -1.7467e-02,\n",
       "          -3.3675e-01, -1.3343e-01, -3.0798e-01, -2.9590e-01,  3.7961e-01],\n",
       "         [-3.1275e-02,  1.5501e-01, -5.0019e-02, -2.0230e-01, -1.7696e-01,\n",
       "           1.4355e-01,  3.8513e-01, -1.4374e-01,  5.4120e-02,  2.8015e-02,\n",
       "           1.6499e-02, -3.2897e-02,  2.2811e-01, -3.1615e-01, -2.7160e-02,\n",
       "          -3.3424e-01, -1.5301e-01, -2.9446e-01, -3.0190e-01,  3.5629e-01],\n",
       "         [-3.4698e-02,  1.1764e-01, -4.7341e-02, -1.9085e-01, -1.8176e-01,\n",
       "           1.5075e-01,  3.4970e-01, -1.1061e-01,  4.2905e-02,  2.2629e-02,\n",
       "           7.2409e-03, -4.3968e-02,  2.1482e-01, -2.9328e-01, -1.6573e-02,\n",
       "          -2.8272e-01, -1.4475e-01, -2.9688e-01, -2.7190e-01,  3.2338e-01],\n",
       "         [-4.7680e-02,  1.4600e-01, -7.2861e-02, -1.5249e-01, -1.5529e-01,\n",
       "           1.1956e-01,  3.3920e-01, -2.0877e-01,  5.0614e-02,  1.8789e-02,\n",
       "           7.8706e-02, -3.5017e-02,  2.0781e-01, -3.0052e-01, -1.9276e-02,\n",
       "          -3.0867e-01, -1.1451e-01, -1.7456e-01, -2.6768e-01,  4.1150e-01],\n",
       "         [-3.5095e-02,  1.7401e-01, -4.3081e-02, -2.2955e-01, -2.0121e-01,\n",
       "           1.5752e-01,  4.1405e-01, -1.7220e-01,  7.0029e-02,  4.3348e-02,\n",
       "           2.3083e-02, -3.9995e-02,  2.5594e-01, -3.2642e-01, -2.4258e-02,\n",
       "          -3.9469e-01, -1.4356e-01, -3.0545e-01, -3.2786e-01,  4.1945e-01],\n",
       "         [-3.8977e-02,  1.5505e-01, -5.5843e-02, -1.6536e-01, -1.8570e-01,\n",
       "           1.6037e-01,  3.4382e-01, -1.6267e-01,  4.9799e-02,  2.8553e-02,\n",
       "           5.8355e-02, -3.0300e-02,  2.2703e-01, -2.9783e-01, -1.5526e-02,\n",
       "          -3.5993e-01, -1.1961e-01, -2.2041e-01, -2.9216e-01,  4.0341e-01]],\n",
       "\n",
       "        [[-9.8423e-02,  1.2197e-01, -1.3051e-01, -1.3009e-01, -1.3241e-01,\n",
       "           8.0376e-02,  3.3665e-01, -1.5353e-01,  2.9265e-02, -6.0822e-02,\n",
       "           8.7852e-02, -7.7320e-02,  2.3564e-01, -3.7154e-01,  9.2458e-03,\n",
       "          -2.3674e-01, -1.9326e-01, -3.1605e-01, -2.5381e-01,  3.7013e-01],\n",
       "         [-8.3067e-02,  7.5877e-02, -1.2006e-01, -1.0596e-01, -1.3264e-01,\n",
       "           8.0004e-02,  3.3952e-01, -1.5397e-01,  1.6523e-02, -8.6442e-02,\n",
       "           1.0837e-01, -6.2063e-02,  2.3868e-01, -3.7762e-01,  1.5140e-02,\n",
       "          -2.6502e-01, -1.9744e-01, -3.1177e-01, -2.3853e-01,  3.7885e-01],\n",
       "         [-8.1150e-02,  7.3798e-02, -1.1727e-01, -1.0325e-01, -1.3430e-01,\n",
       "           8.0293e-02,  3.3825e-01, -1.5533e-01,  1.8865e-02, -8.6845e-02,\n",
       "           1.0876e-01, -6.0081e-02,  2.3695e-01, -3.7598e-01,  1.5199e-02,\n",
       "          -2.6829e-01, -1.9555e-01, -3.0893e-01, -2.3762e-01,  3.8018e-01],\n",
       "         [-1.0786e-01,  8.5001e-02, -1.3079e-01, -1.4535e-01, -1.7189e-01,\n",
       "           1.0488e-01,  4.0062e-01, -1.8188e-01,  3.3439e-02, -1.0306e-01,\n",
       "           1.2607e-01, -5.9652e-02,  3.0202e-01, -4.4818e-01,  1.7979e-02,\n",
       "          -3.4597e-01, -2.0165e-01, -3.5665e-01, -3.0666e-01,  4.9541e-01],\n",
       "         [-1.0921e-01,  8.5762e-02, -1.3297e-01, -1.4179e-01, -1.7319e-01,\n",
       "           1.0181e-01,  3.9951e-01, -1.8229e-01,  3.5489e-02, -1.0194e-01,\n",
       "           1.2736e-01, -5.9438e-02,  3.0252e-01, -4.5011e-01,  1.8041e-02,\n",
       "          -3.4627e-01, -2.0416e-01, -3.5604e-01, -3.0982e-01,  4.9501e-01],\n",
       "         [-1.0872e-01,  8.7021e-02, -1.3105e-01, -1.4692e-01, -1.7126e-01,\n",
       "           1.0486e-01,  4.0340e-01, -1.8159e-01,  3.3606e-02, -1.0066e-01,\n",
       "           1.2395e-01, -6.0847e-02,  3.0149e-01, -4.4863e-01,  1.8031e-02,\n",
       "          -3.4405e-01, -2.0232e-01, -3.6008e-01, -3.0803e-01,  4.9268e-01],\n",
       "         [-1.0863e-01,  8.5336e-02, -1.3121e-01, -1.4777e-01, -1.7044e-01,\n",
       "           1.0456e-01,  4.0316e-01, -1.8457e-01,  3.2356e-02, -1.0234e-01,\n",
       "           1.2483e-01, -6.1335e-02,  3.0153e-01, -4.4871e-01,  1.8454e-02,\n",
       "          -3.4364e-01, -2.0202e-01, -3.5920e-01, -3.0486e-01,  4.9427e-01],\n",
       "         [-1.1616e-01,  1.1036e-01, -1.3746e-01, -1.4802e-01, -1.5623e-01,\n",
       "           8.8621e-02,  3.5884e-01, -1.6235e-01,  2.3053e-02, -8.4885e-02,\n",
       "           1.0773e-01, -7.3221e-02,  2.8226e-01, -4.2006e-01,  1.7293e-02,\n",
       "          -2.8081e-01, -1.9873e-01, -3.3279e-01, -2.8917e-01,  4.4323e-01],\n",
       "         [-7.6782e-02,  2.6062e-02, -1.0362e-01, -3.4249e-02, -1.2370e-01,\n",
       "           5.5288e-02,  2.6035e-01, -1.6591e-01,  3.6907e-02, -1.0610e-01,\n",
       "           1.5069e-01, -2.8787e-02,  2.1615e-01, -3.2383e-01,  1.6055e-02,\n",
       "          -2.8210e-01, -1.2840e-01, -1.9302e-01, -1.9414e-01,  4.1130e-01],\n",
       "         [-9.2199e-02,  9.9266e-02, -1.2666e-01, -1.3556e-01, -1.4518e-01,\n",
       "           9.8561e-02,  3.8652e-01, -1.7396e-01,  3.5504e-02, -7.8358e-02,\n",
       "           1.0348e-01, -6.7659e-02,  2.5750e-01, -4.0332e-01,  1.0286e-02,\n",
       "          -2.9790e-01, -1.9882e-01, -3.4967e-01, -2.7096e-01,  4.2069e-01]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head attention block simply applies multiple attention heads, then concatenates the outputs and applies a single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        # in practice, d_model == d_feature * n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        # Note that this is very inefficient:\n",
    "        # I am merely implementing the heads separately because it is \n",
    "        # easier to understand this way\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"output of single head\")\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 160])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1361,  0.1967, -0.0143,  ..., -0.0137, -0.0800,  0.3766],\n",
       "         [-0.1109,  0.1385, -0.1142,  ...,  0.0575, -0.0618,  0.3560],\n",
       "         [-0.1316,  0.1787, -0.0577,  ..., -0.0099, -0.0858,  0.3920],\n",
       "         ...,\n",
       "         [-0.1761,  0.1899, -0.0330,  ...,  0.0197, -0.0750,  0.3744],\n",
       "         [-0.1167,  0.1407, -0.0622,  ...,  0.0369, -0.0651,  0.4067],\n",
       "         [-0.1415,  0.1933, -0.0618,  ..., -0.0101, -0.0888,  0.3898]],\n",
       "\n",
       "        [[-0.2291,  0.1599, -0.0159,  ...,  0.0174, -0.0642,  0.3414],\n",
       "         [-0.2868,  0.1225, -0.0264,  ...,  0.0438,  0.0078,  0.3615],\n",
       "         [-0.2686,  0.1321, -0.0515,  ...,  0.0715, -0.0287,  0.3564],\n",
       "         ...,\n",
       "         [-0.2327,  0.1424, -0.0644,  ...,  0.0858, -0.0364,  0.3280],\n",
       "         [-0.2613,  0.0851, -0.0637,  ...,  0.0664, -0.0199,  0.3532],\n",
       "         [-0.2519,  0.1490, -0.0324,  ...,  0.0291, -0.0154,  0.3227]],\n",
       "\n",
       "        [[-0.1771,  0.2177, -0.0696,  ...,  0.0302, -0.0724,  0.3467],\n",
       "         [-0.1663,  0.2216, -0.0125,  ...,  0.0501, -0.0598,  0.3514],\n",
       "         [-0.1117,  0.2187, -0.0392,  ...,  0.0187, -0.1227,  0.3624],\n",
       "         ...,\n",
       "         [-0.1994,  0.2164, -0.0470,  ...,  0.0700, -0.0268,  0.3242],\n",
       "         [-0.2142,  0.2140, -0.0931,  ...,  0.0255, -0.0555,  0.3525],\n",
       "         [-0.1788,  0.2167, -0.0483,  ...,  0.0503, -0.0595,  0.3615]],\n",
       "\n",
       "        [[-0.1468,  0.1501, -0.0274,  ...,  0.0864, -0.0707,  0.3847],\n",
       "         [-0.1437,  0.1482, -0.0423,  ...,  0.0553, -0.0766,  0.3615],\n",
       "         [-0.1521,  0.1481, -0.0031,  ...,  0.0623, -0.0772,  0.4108],\n",
       "         ...,\n",
       "         [-0.1658,  0.1378, -0.0166,  ...,  0.1021, -0.0490,  0.3670],\n",
       "         [-0.1911,  0.1232, -0.0216,  ...,  0.0740, -0.0417,  0.3757],\n",
       "         [-0.1615,  0.1781,  0.0055,  ...,  0.0599, -0.0842,  0.3838]],\n",
       "\n",
       "        [[-0.1502,  0.1605, -0.0480,  ...,  0.0587, -0.1487,  0.4282],\n",
       "         [-0.1393,  0.1418, -0.0913,  ...,  0.0688, -0.1194,  0.4293],\n",
       "         [-0.1419,  0.1605, -0.0898,  ...,  0.0292, -0.1337,  0.3888],\n",
       "         ...,\n",
       "         [-0.1687,  0.1324, -0.0565,  ...,  0.0490, -0.0873,  0.3849],\n",
       "         [-0.1206,  0.1387, -0.0462,  ...,  0.0699, -0.1503,  0.4066],\n",
       "         [-0.1272,  0.1248, -0.0562,  ...,  0.0626, -0.1271,  0.4211]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core components in place, implementing the encoder is pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.41.png?w=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of the following components:\n",
    "- A multi-head attention block\n",
    "- A simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the individual attention heads\n",
    "logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization is similar to batch normalization, but normalizes across the feature dimension instead of the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/01/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-01-11-11.48.12.png?w=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder just stacks these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention output\")\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9765e+00,  6.0555e-01,  5.7684e-01,  ...,  2.7611e+00,\n",
       "          -5.1025e-01,  2.7049e+00],\n",
       "         [ 1.6280e+00,  2.1314e-01,  2.5232e+00,  ...,  1.5289e-01,\n",
       "          -1.0746e+00,  2.3660e+00],\n",
       "         [ 1.0024e+00,  9.7015e-01,  1.1129e+00,  ...,  7.8212e-01,\n",
       "          -1.6309e+00,  3.1498e-02],\n",
       "         ...,\n",
       "         [ 5.2415e-01,  4.5764e-01,  9.8830e-01,  ...,  2.7500e+00,\n",
       "          -5.1298e-01,  7.5724e-01],\n",
       "         [ 5.0665e-01, -5.2399e-01,  8.6294e-01,  ...,  1.4582e+00,\n",
       "          -1.0289e+00,  2.0130e+00],\n",
       "         [ 1.3012e+00,  7.2873e-01,  1.4577e+00,  ...,  8.2640e-01,\n",
       "          -2.1387e+00,  1.8953e+00]],\n",
       "\n",
       "        [[ 1.3996e+00,  8.3388e-03,  4.9115e-01,  ...,  2.3502e+00,\n",
       "          -1.8098e+00,  2.5786e+00],\n",
       "         [ 1.4870e+00,  5.6864e-01,  1.5955e+00,  ...,  9.2919e-01,\n",
       "          -1.7983e+00,  2.7972e+00],\n",
       "         [ 1.6345e+00,  1.2660e-01,  1.0261e+00,  ...,  2.5134e+00,\n",
       "           5.9002e-01,  1.4710e+00],\n",
       "         ...,\n",
       "         [ 1.2663e+00,  4.2315e-01,  1.2314e+00,  ...,  4.3199e-01,\n",
       "          -1.2662e+00,  1.5332e+00],\n",
       "         [ 1.9611e+00, -4.8261e-01,  1.4666e+00,  ...,  5.2811e-01,\n",
       "          -1.7328e+00,  2.0733e+00],\n",
       "         [ 2.0614e+00,  2.9268e-01,  6.6514e-01,  ...,  8.1028e-01,\n",
       "           1.1095e-01,  1.5794e+00]],\n",
       "\n",
       "        [[ 3.2250e-01,  6.1192e-02,  1.4283e+00,  ...,  7.5319e-01,\n",
       "           2.0343e-01,  2.6024e-03],\n",
       "         [ 1.1291e+00,  7.1887e-01, -7.9967e-01,  ..., -7.0995e-01,\n",
       "          -1.8307e+00,  1.5308e+00],\n",
       "         [ 1.9453e+00,  6.9439e-01,  1.0791e+00,  ...,  1.1790e+00,\n",
       "          -1.1375e+00,  2.0504e+00],\n",
       "         ...,\n",
       "         [ 7.8128e-01,  9.1591e-01,  2.1534e+00,  ...,  1.4581e+00,\n",
       "          -2.1792e+00,  9.9935e-01],\n",
       "         [ 1.5031e+00, -2.7549e-01,  1.9382e+00,  ...,  4.1924e-01,\n",
       "          -1.1981e+00,  3.0674e+00],\n",
       "         [ 1.1006e+00,  1.1394e+00,  4.9833e-02,  ...,  1.0832e+00,\n",
       "          -1.0903e+00,  2.1628e+00]],\n",
       "\n",
       "        [[ 8.2770e-01,  2.7215e-02, -9.8626e-01,  ..., -7.7264e-01,\n",
       "          -1.8087e+00,  3.0275e+00],\n",
       "         [ 2.4003e+00, -6.7281e-01,  8.2076e-01,  ...,  1.4266e+00,\n",
       "          -2.1982e+00,  3.3121e+00],\n",
       "         [ 2.0155e+00,  8.8487e-01,  1.2083e+00,  ...,  8.9198e-01,\n",
       "          -5.0986e-01,  2.3244e+00],\n",
       "         ...,\n",
       "         [ 1.6427e+00,  1.0732e-01,  1.9801e+00,  ..., -5.2428e-01,\n",
       "          -2.3256e+00,  2.7196e+00],\n",
       "         [ 1.8589e+00, -4.2761e-03,  1.8523e+00,  ...,  1.6956e+00,\n",
       "          -2.4065e+00,  2.9872e+00],\n",
       "         [ 2.6166e+00,  7.1239e-01,  7.3602e-01,  ...,  2.7105e-01,\n",
       "          -2.3598e+00,  6.2951e-01]],\n",
       "\n",
       "        [[ 1.3791e+00,  3.6285e-01,  1.5873e+00,  ...,  1.1559e+00,\n",
       "          -1.3766e-01,  2.5151e+00],\n",
       "         [ 8.6774e-01,  7.6326e-01,  2.0110e+00,  ...,  1.4122e+00,\n",
       "          -1.4105e+00,  3.0316e+00],\n",
       "         [ 1.4868e-01,  3.5826e-01,  2.0241e+00,  ...,  2.0641e+00,\n",
       "          -1.0768e+00,  1.9778e+00],\n",
       "         ...,\n",
       "         [ 1.7447e+00, -3.1353e-01,  3.8002e-01,  ...,  1.3654e+00,\n",
       "          -1.9862e+00,  1.7614e+00],\n",
       "         [ 1.3890e+00, -9.5069e-01,  1.8355e+00,  ...,  1.1227e+00,\n",
       "          -8.3943e-01,  1.5598e+00],\n",
       "         [ 2.6745e-01,  2.9384e-01,  2.0174e+00,  ...,  1.4125e+00,\n",
       "          -9.0330e-01,  1.8357e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of 6 consecutive encoder blocks, so can simply be implemented like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512,\n",
    "                 n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is mostly the same as the encoder. There's just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i1.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.47.png?w=287)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys and values are the outputs of the encoder, and the queries are the outputs of the multi-head attention over the target entence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Apply attention to inputs\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Apply attention to the encoder outputs and outputs of the previous layer\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6213e+00, -1.5202e+00, -1.5732e+00,  ...,  1.0225e+00,\n",
       "           1.5174e+00,  7.7989e-01],\n",
       "         [-2.1316e+00, -1.1355e+00, -1.2760e+00,  ...,  7.8845e-01,\n",
       "           1.9173e+00,  2.3724e+00],\n",
       "         [-2.3049e+00, -1.8938e-01, -2.6571e+00,  ...,  3.9799e-01,\n",
       "           3.4069e+00,  1.1630e+00],\n",
       "         ...,\n",
       "         [-2.2333e+00, -2.5128e+00, -1.7321e+00,  ..., -7.5708e-01,\n",
       "           3.5582e+00,  1.4875e+00],\n",
       "         [-1.1305e+00, -1.1589e+00, -1.5274e+00,  ...,  1.4933e+00,\n",
       "           2.9591e+00,  8.0061e-01],\n",
       "         [-1.2231e+00, -2.4310e+00, -1.9051e+00,  ...,  1.6243e+00,\n",
       "           3.0201e+00,  2.1192e+00]],\n",
       "\n",
       "        [[-1.5041e+00, -1.1104e+00, -2.3338e+00,  ...,  1.0043e+00,\n",
       "           2.7498e+00,  5.9983e-01],\n",
       "         [-2.2018e+00, -8.9073e-01, -2.0031e+00,  ...,  9.3639e-01,\n",
       "           2.3343e+00,  1.3269e+00],\n",
       "         [-2.0905e+00, -1.9472e+00, -1.4471e+00,  ...,  8.9177e-01,\n",
       "           2.6886e+00,  4.6184e-01],\n",
       "         ...,\n",
       "         [-1.8132e+00,  6.7970e-01, -2.4250e+00,  ..., -3.3134e-03,\n",
       "           3.5993e+00,  1.1658e+00],\n",
       "         [-2.8891e+00, -3.1835e-01, -1.2308e+00,  ...,  8.3808e-01,\n",
       "           3.5193e+00,  1.7243e+00],\n",
       "         [-1.5543e+00, -4.3663e-01, -1.3130e+00,  ...,  5.4717e-01,\n",
       "           3.4714e+00,  1.2663e-01]],\n",
       "\n",
       "        [[-2.0118e+00, -1.3442e+00, -6.9594e-01,  ..., -1.2022e-01,\n",
       "           1.9530e+00,  1.5130e+00],\n",
       "         [-1.6069e+00, -6.3984e-01, -1.4216e+00,  ...,  6.8430e-01,\n",
       "           3.1099e+00,  2.9096e-01],\n",
       "         [-1.6559e+00, -6.9771e-01, -1.7534e+00,  ...,  1.2302e-02,\n",
       "           3.7716e+00,  1.3894e+00],\n",
       "         ...,\n",
       "         [-1.5797e+00, -1.0893e+00, -1.3942e+00,  ...,  1.1252e+00,\n",
       "           2.9508e+00,  9.2879e-01],\n",
       "         [-1.8938e+00, -1.0252e+00, -1.8955e+00,  ...,  5.2857e-01,\n",
       "           3.0339e+00,  8.3272e-01],\n",
       "         [-7.1889e-01, -9.7388e-01, -1.4640e+00,  ..., -1.6155e-01,\n",
       "           3.1957e+00,  2.0792e+00]],\n",
       "\n",
       "        [[-1.4331e+00, -5.6701e-01, -1.9019e+00,  ...,  6.3124e-01,\n",
       "           2.9646e+00,  1.1110e+00],\n",
       "         [ 1.5507e-02, -9.5473e-01, -5.0531e-01,  ...,  1.4534e+00,\n",
       "           3.5644e+00,  2.0573e+00],\n",
       "         [-1.0747e+00, -2.4799e-01, -1.2057e+00,  ...,  6.5968e-01,\n",
       "           2.2911e+00, -1.3664e-01],\n",
       "         ...,\n",
       "         [-1.6826e+00, -1.9170e+00, -1.6753e+00,  ...,  1.3202e+00,\n",
       "           3.6620e+00,  1.7967e+00],\n",
       "         [-9.6600e-01, -7.5734e-01, -1.5075e+00,  ...,  1.8786e+00,\n",
       "           2.3437e+00,  3.8177e-01],\n",
       "         [ 8.5536e-01, -1.0813e+00, -1.6758e+00,  ...,  4.0425e-01,\n",
       "           4.6109e+00,  9.5835e-02]],\n",
       "\n",
       "        [[-1.7907e+00, -6.4781e-01, -2.2684e+00,  ...,  1.2777e+00,\n",
       "           4.4339e+00,  7.0131e-02],\n",
       "         [-2.9003e+00, -1.2833e+00, -1.7777e+00,  ...,  6.7385e-01,\n",
       "           3.8419e+00,  8.7303e-01],\n",
       "         [-1.9120e+00, -1.7718e-01, -2.2390e+00,  ...,  1.0502e+00,\n",
       "           4.2289e+00,  1.3151e+00],\n",
       "         ...,\n",
       "         [-2.4716e+00,  4.5530e-01, -2.3887e+00,  ..., -1.1988e-01,\n",
       "           3.7637e+00,  7.1965e-01],\n",
       "         [-1.0169e+00, -1.4527e+00, -1.9651e+00,  ...,  9.2473e-01,\n",
       "           3.6673e+00,  8.8438e-01],\n",
       "         [-2.3239e+00,  7.0408e-02, -1.5554e+00,  ...,  9.5516e-01,\n",
       "           4.2436e+00,  1.1772e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the decoder is just a stack of the underlying block so is simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks are just simple matrix multiplications: therefore they don't have any notion of order! The Transformer explicitly adds positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=160\n",
    "max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 8.9125e-01, 7.9433e-01, 7.0795e-01, 6.3096e-01, 5.6234e-01,\n",
       "        5.0119e-01, 4.4668e-01, 3.9811e-01, 3.5481e-01, 3.1623e-01, 2.8184e-01,\n",
       "        2.5119e-01, 2.2387e-01, 1.9953e-01, 1.7783e-01, 1.5849e-01, 1.4125e-01,\n",
       "        1.2589e-01, 1.1220e-01, 1.0000e-01, 8.9125e-02, 7.9433e-02, 7.0795e-02,\n",
       "        6.3096e-02, 5.6234e-02, 5.0119e-02, 4.4668e-02, 3.9811e-02, 3.5481e-02,\n",
       "        3.1623e-02, 2.8184e-02, 2.5119e-02, 2.2387e-02, 1.9953e-02, 1.7783e-02,\n",
       "        1.5849e-02, 1.4125e-02, 1.2589e-02, 1.1220e-02, 1.0000e-02, 8.9125e-03,\n",
       "        7.9433e-03, 7.0795e-03, 6.3096e-03, 5.6234e-03, 5.0119e-03, 4.4668e-03,\n",
       "        3.9811e-03, 3.5481e-03, 3.1623e-03, 2.8184e-03, 2.5119e-03, 2.2387e-03,\n",
       "        1.9953e-03, 1.7783e-03, 1.5849e-03, 1.4125e-03, 1.2589e-03, 1.1220e-03,\n",
       "        1.0000e-03, 8.9125e-04, 7.9433e-04, 7.0795e-04, 6.3096e-04, 5.6234e-04,\n",
       "        5.0119e-04, 4.4668e-04, 3.9811e-04, 3.5481e-04, 3.1623e-04, 2.8184e-04,\n",
       "        2.5119e-04, 2.2387e-04, 1.9953e-04, 1.7783e-04, 1.5849e-04, 1.4125e-04,\n",
       "        1.2589e-04, 1.1220e-04])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "#torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 80])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(max_len, d_model)[:, 0::2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 80])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(max_len, d_model)[:, 1::2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(input=torch.ones(20), dim=0)[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.],\n",
       "        [  1.],\n",
       "        [  2.],\n",
       "        [  3.],\n",
       "        [  4.],\n",
       "        [  5.],\n",
       "        [  6.],\n",
       "        [  7.],\n",
       "        [  8.],\n",
       "        [  9.],\n",
       "        [ 10.],\n",
       "        [ 11.],\n",
       "        [ 12.],\n",
       "        [ 13.],\n",
       "        [ 14.],\n",
       "        [ 15.],\n",
       "        [ 16.],\n",
       "        [ 17.],\n",
       "        [ 18.],\n",
       "        [ 19.],\n",
       "        [ 20.],\n",
       "        [ 21.],\n",
       "        [ 22.],\n",
       "        [ 23.],\n",
       "        [ 24.],\n",
       "        [ 25.],\n",
       "        [ 26.],\n",
       "        [ 27.],\n",
       "        [ 28.],\n",
       "        [ 29.],\n",
       "        [ 30.],\n",
       "        [ 31.],\n",
       "        [ 32.],\n",
       "        [ 33.],\n",
       "        [ 34.],\n",
       "        [ 35.],\n",
       "        [ 36.],\n",
       "        [ 37.],\n",
       "        [ 38.],\n",
       "        [ 39.],\n",
       "        [ 40.],\n",
       "        [ 41.],\n",
       "        [ 42.],\n",
       "        [ 43.],\n",
       "        [ 44.],\n",
       "        [ 45.],\n",
       "        [ 46.],\n",
       "        [ 47.],\n",
       "        [ 48.],\n",
       "        [ 49.],\n",
       "        [ 50.],\n",
       "        [ 51.],\n",
       "        [ 52.],\n",
       "        [ 53.],\n",
       "        [ 54.],\n",
       "        [ 55.],\n",
       "        [ 56.],\n",
       "        [ 57.],\n",
       "        [ 58.],\n",
       "        [ 59.],\n",
       "        [ 60.],\n",
       "        [ 61.],\n",
       "        [ 62.],\n",
       "        [ 63.],\n",
       "        [ 64.],\n",
       "        [ 65.],\n",
       "        [ 66.],\n",
       "        [ 67.],\n",
       "        [ 68.],\n",
       "        [ 69.],\n",
       "        [ 70.],\n",
       "        [ 71.],\n",
       "        [ 72.],\n",
       "        [ 73.],\n",
       "        [ 74.],\n",
       "        [ 75.],\n",
       "        [ 76.],\n",
       "        [ 77.],\n",
       "        [ 78.],\n",
       "        [ 79.],\n",
       "        [ 80.],\n",
       "        [ 81.],\n",
       "        [ 82.],\n",
       "        [ 83.],\n",
       "        [ 84.],\n",
       "        [ 85.],\n",
       "        [ 86.],\n",
       "        [ 87.],\n",
       "        [ 88.],\n",
       "        [ 89.],\n",
       "        [ 90.],\n",
       "        [ 91.],\n",
       "        [ 92.],\n",
       "        [ 93.],\n",
       "        [ 94.],\n",
       "        [ 95.],\n",
       "        [ 96.],\n",
       "        [ 97.],\n",
       "        [ 98.],\n",
       "        [ 99.],\n",
       "        [100.],\n",
       "        [101.],\n",
       "        [102.],\n",
       "        [103.],\n",
       "        [104.],\n",
       "        [105.],\n",
       "        [106.],\n",
       "        [107.],\n",
       "        [108.],\n",
       "        [109.],\n",
       "        [110.],\n",
       "        [111.],\n",
       "        [112.],\n",
       "        [113.],\n",
       "        [114.],\n",
       "        [115.],\n",
       "        [116.],\n",
       "        [117.],\n",
       "        [118.],\n",
       "        [119.],\n",
       "        [120.],\n",
       "        [121.],\n",
       "        [122.],\n",
       "        [123.],\n",
       "        [124.],\n",
       "        [125.],\n",
       "        [126.],\n",
       "        [127.],\n",
       "        [128.],\n",
       "        [129.],\n",
       "        [130.],\n",
       "        [131.],\n",
       "        [132.],\n",
       "        [133.],\n",
       "        [134.],\n",
       "        [135.],\n",
       "        [136.],\n",
       "        [137.],\n",
       "        [138.],\n",
       "        [139.],\n",
       "        [140.],\n",
       "        [141.],\n",
       "        [142.],\n",
       "        [143.],\n",
       "        [144.],\n",
       "        [145.],\n",
       "        [146.],\n",
       "        [147.],\n",
       "        [148.],\n",
       "        [149.],\n",
       "        [150.],\n",
       "        [151.],\n",
       "        [152.],\n",
       "        [153.],\n",
       "        [154.],\n",
       "        [155.],\n",
       "        [156.],\n",
       "        [157.],\n",
       "        [158.],\n",
       "        [159.],\n",
       "        [160.],\n",
       "        [161.],\n",
       "        [162.],\n",
       "        [163.],\n",
       "        [164.],\n",
       "        [165.],\n",
       "        [166.],\n",
       "        [167.],\n",
       "        [168.],\n",
       "        [169.],\n",
       "        [170.],\n",
       "        [171.],\n",
       "        [172.],\n",
       "        [173.],\n",
       "        [174.],\n",
       "        [175.],\n",
       "        [176.],\n",
       "        [177.],\n",
       "        [178.],\n",
       "        [179.],\n",
       "        [180.],\n",
       "        [181.],\n",
       "        [182.],\n",
       "        [183.],\n",
       "        [184.],\n",
       "        [185.],\n",
       "        [186.],\n",
       "        [187.],\n",
       "        [188.],\n",
       "        [189.],\n",
       "        [190.],\n",
       "        [191.],\n",
       "        [192.],\n",
       "        [193.],\n",
       "        [194.],\n",
       "        [195.],\n",
       "        [196.],\n",
       "        [197.],\n",
       "        [198.],\n",
       "        [199.],\n",
       "        [200.],\n",
       "        [201.],\n",
       "        [202.],\n",
       "        [203.],\n",
       "        [204.],\n",
       "        [205.],\n",
       "        [206.],\n",
       "        [207.],\n",
       "        [208.],\n",
       "        [209.],\n",
       "        [210.],\n",
       "        [211.],\n",
       "        [212.],\n",
       "        [213.],\n",
       "        [214.],\n",
       "        [215.],\n",
       "        [216.],\n",
       "        [217.],\n",
       "        [218.],\n",
       "        [219.],\n",
       "        [220.],\n",
       "        [221.],\n",
       "        [222.],\n",
       "        [223.],\n",
       "        [224.],\n",
       "        [225.],\n",
       "        [226.],\n",
       "        [227.],\n",
       "        [228.],\n",
       "        [229.],\n",
       "        [230.],\n",
       "        [231.],\n",
       "        [232.],\n",
       "        [233.],\n",
       "        [234.],\n",
       "        [235.],\n",
       "        [236.],\n",
       "        [237.],\n",
       "        [238.],\n",
       "        [239.],\n",
       "        [240.],\n",
       "        [241.],\n",
       "        [242.],\n",
       "        [243.],\n",
       "        [244.],\n",
       "        [245.],\n",
       "        [246.],\n",
       "        [247.],\n",
       "        [248.],\n",
       "        [249.],\n",
       "        [250.],\n",
       "        [251.],\n",
       "        [252.],\n",
       "        [253.],\n",
       "        [254.],\n",
       "        [255.],\n",
       "        [256.],\n",
       "        [257.],\n",
       "        [258.],\n",
       "        [259.],\n",
       "        [260.],\n",
       "        [261.],\n",
       "        [262.],\n",
       "        [263.],\n",
       "        [264.],\n",
       "        [265.],\n",
       "        [266.],\n",
       "        [267.],\n",
       "        [268.],\n",
       "        [269.],\n",
       "        [270.],\n",
       "        [271.],\n",
       "        [272.],\n",
       "        [273.],\n",
       "        [274.],\n",
       "        [275.],\n",
       "        [276.],\n",
       "        [277.],\n",
       "        [278.],\n",
       "        [279.],\n",
       "        [280.],\n",
       "        [281.],\n",
       "        [282.],\n",
       "        [283.],\n",
       "        [284.],\n",
       "        [285.],\n",
       "        [286.],\n",
       "        [287.],\n",
       "        [288.],\n",
       "        [289.],\n",
       "        [290.],\n",
       "        [291.],\n",
       "        [292.],\n",
       "        [293.],\n",
       "        [294.],\n",
       "        [295.],\n",
       "        [296.],\n",
       "        [297.],\n",
       "        [298.],\n",
       "        [299.],\n",
       "        [300.],\n",
       "        [301.],\n",
       "        [302.],\n",
       "        [303.],\n",
       "        [304.],\n",
       "        [305.],\n",
       "        [306.],\n",
       "        [307.],\n",
       "        [308.],\n",
       "        [309.],\n",
       "        [310.],\n",
       "        [311.],\n",
       "        [312.],\n",
       "        [313.],\n",
       "        [314.],\n",
       "        [315.],\n",
       "        [316.],\n",
       "        [317.],\n",
       "        [318.],\n",
       "        [319.],\n",
       "        [320.],\n",
       "        [321.],\n",
       "        [322.],\n",
       "        [323.],\n",
       "        [324.],\n",
       "        [325.],\n",
       "        [326.],\n",
       "        [327.],\n",
       "        [328.],\n",
       "        [329.],\n",
       "        [330.],\n",
       "        [331.],\n",
       "        [332.],\n",
       "        [333.],\n",
       "        [334.],\n",
       "        [335.],\n",
       "        [336.],\n",
       "        [337.],\n",
       "        [338.],\n",
       "        [339.],\n",
       "        [340.],\n",
       "        [341.],\n",
       "        [342.],\n",
       "        [343.],\n",
       "        [344.],\n",
       "        [345.],\n",
       "        [346.],\n",
       "        [347.],\n",
       "        [348.],\n",
       "        [349.],\n",
       "        [350.],\n",
       "        [351.],\n",
       "        [352.],\n",
       "        [353.],\n",
       "        [354.],\n",
       "        [355.],\n",
       "        [356.],\n",
       "        [357.],\n",
       "        [358.],\n",
       "        [359.],\n",
       "        [360.],\n",
       "        [361.],\n",
       "        [362.],\n",
       "        [363.],\n",
       "        [364.],\n",
       "        [365.],\n",
       "        [366.],\n",
       "        [367.],\n",
       "        [368.],\n",
       "        [369.],\n",
       "        [370.],\n",
       "        [371.],\n",
       "        [372.],\n",
       "        [373.],\n",
       "        [374.],\n",
       "        [375.],\n",
       "        [376.],\n",
       "        [377.],\n",
       "        [378.],\n",
       "        [379.],\n",
       "        [380.],\n",
       "        [381.],\n",
       "        [382.],\n",
       "        [383.],\n",
       "        [384.],\n",
       "        [385.],\n",
       "        [386.],\n",
       "        [387.],\n",
       "        [388.],\n",
       "        [389.],\n",
       "        [390.],\n",
       "        [391.],\n",
       "        [392.],\n",
       "        [393.],\n",
       "        [394.],\n",
       "        [395.],\n",
       "        [396.],\n",
       "        [397.],\n",
       "        [398.],\n",
       "        [399.],\n",
       "        [400.],\n",
       "        [401.],\n",
       "        [402.],\n",
       "        [403.],\n",
       "        [404.],\n",
       "        [405.],\n",
       "        [406.],\n",
       "        [407.],\n",
       "        [408.],\n",
       "        [409.],\n",
       "        [410.],\n",
       "        [411.],\n",
       "        [412.],\n",
       "        [413.],\n",
       "        [414.],\n",
       "        [415.],\n",
       "        [416.],\n",
       "        [417.],\n",
       "        [418.],\n",
       "        [419.],\n",
       "        [420.],\n",
       "        [421.],\n",
       "        [422.],\n",
       "        [423.],\n",
       "        [424.],\n",
       "        [425.],\n",
       "        [426.],\n",
       "        [427.],\n",
       "        [428.],\n",
       "        [429.],\n",
       "        [430.],\n",
       "        [431.],\n",
       "        [432.],\n",
       "        [433.],\n",
       "        [434.],\n",
       "        [435.],\n",
       "        [436.],\n",
       "        [437.],\n",
       "        [438.],\n",
       "        [439.],\n",
       "        [440.],\n",
       "        [441.],\n",
       "        [442.],\n",
       "        [443.],\n",
       "        [444.],\n",
       "        [445.],\n",
       "        [446.],\n",
       "        [447.],\n",
       "        [448.],\n",
       "        [449.],\n",
       "        [450.],\n",
       "        [451.],\n",
       "        [452.],\n",
       "        [453.],\n",
       "        [454.],\n",
       "        [455.],\n",
       "        [456.],\n",
       "        [457.],\n",
       "        [458.],\n",
       "        [459.],\n",
       "        [460.],\n",
       "        [461.],\n",
       "        [462.],\n",
       "        [463.],\n",
       "        [464.],\n",
       "        [465.],\n",
       "        [466.],\n",
       "        [467.],\n",
       "        [468.],\n",
       "        [469.],\n",
       "        [470.],\n",
       "        [471.],\n",
       "        [472.],\n",
       "        [473.],\n",
       "        [474.],\n",
       "        [475.],\n",
       "        [476.],\n",
       "        [477.],\n",
       "        [478.],\n",
       "        [479.],\n",
       "        [480.],\n",
       "        [481.],\n",
       "        [482.],\n",
       "        [483.],\n",
       "        [484.],\n",
       "        [485.],\n",
       "        [486.],\n",
       "        [487.],\n",
       "        [488.],\n",
       "        [489.],\n",
       "        [490.],\n",
       "        [491.],\n",
       "        [492.],\n",
       "        [493.],\n",
       "        [494.],\n",
       "        [495.],\n",
       "        [496.],\n",
       "        [497.],\n",
       "        [498.],\n",
       "        [499.],\n",
       "        [500.],\n",
       "        [501.],\n",
       "        [502.],\n",
       "        [503.],\n",
       "        [504.],\n",
       "        [505.],\n",
       "        [506.],\n",
       "        [507.],\n",
       "        [508.],\n",
       "        [509.],\n",
       "        [510.],\n",
       "        [511.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 512).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.1814, -4.3857, -1.5876,  ...,  4.5389,  0.0219,  7.1801],\n",
       "         [-1.2395, -1.5003, -0.2818,  ...,  2.7318, -0.3822,  4.6980],\n",
       "         [ 3.1122, -4.5849,  5.4439,  ...,  3.1859, -0.4301,  4.0371],\n",
       "         ...,\n",
       "         [ 1.0315, -3.7342,  0.7386,  ...,  4.3112,  0.4267,  4.5148],\n",
       "         [ 1.7477, -5.4212, -2.6264,  ...,  3.2503,  1.8450,  1.2794],\n",
       "         [-1.5176, -1.4154,  2.3016,  ...,  6.0029,  0.4483,  3.1732]],\n",
       "\n",
       "        [[-0.8589, -1.1857, -1.4524,  ..., 13.3574,  3.4316,  2.9086],\n",
       "         [-2.1845, -3.5757,  0.9847,  ...,  5.1852, -3.8154,  2.8707],\n",
       "         [ 0.2253, -6.0013,  4.3344,  ...,  5.1273,  1.0284,  3.8588],\n",
       "         ...,\n",
       "         [-3.0157, -1.1752,  0.4322,  ...,  2.8387, -0.3303,  3.1197],\n",
       "         [ 1.0885, -0.5419, -0.1657,  ...,  4.3360,  0.2578,  2.1933],\n",
       "         [-1.4615, -4.1920,  1.6956,  ...,  6.6987,  2.1902,  3.4976]],\n",
       "\n",
       "        [[ 0.8237, -2.3076,  1.0450,  ...,  6.6595, -1.7309, -1.5133],\n",
       "         [ 1.6962, -3.5292, -0.3251,  ...,  3.8539, -0.5088,  4.7821],\n",
       "         [ 4.7293, -3.7605,  3.5794,  ...,  2.5859, -1.7805,  2.9117],\n",
       "         ...,\n",
       "         [ 1.4099, -5.1129, -0.9386,  ...,  7.6519, -0.4538,  0.5043],\n",
       "         [ 2.1198, -8.7028,  1.2167,  ...,  6.7367, -2.9656,  7.9170],\n",
       "         [ 0.3541, -4.9984,  1.8551,  ...,  7.8705,  0.1452,  1.2836]],\n",
       "\n",
       "        [[-3.8347, -0.6470, -0.7554,  ...,  3.5808, -0.3323,  6.4862],\n",
       "         [-0.4940, -2.8429,  0.6287,  ...,  6.1897,  1.3502,  2.1148],\n",
       "         [ 1.1186, -1.9920, -0.7311,  ...,  8.4710,  1.1588, -1.0495],\n",
       "         ...,\n",
       "         [ 0.5285, -0.0827, -0.9502,  ...,  3.7637,  2.9935,  2.3241],\n",
       "         [ 4.1032, -5.8969,  1.6272,  ...,  2.6571, -0.1215,  1.9551],\n",
       "         [-0.5585, -0.8200,  1.7721,  ...,  9.5449,  1.6939, -0.7306]],\n",
       "\n",
       "        [[ 1.3285, -0.2240, -4.0331,  ...,  6.7551, -0.7270,  0.5480],\n",
       "         [-2.2055, -2.4510,  2.7336,  ...,  6.3060,  2.0506,  2.0926],\n",
       "         [-3.3860, -4.2717, -2.5921,  ...,  6.8926, -1.1092,  0.6421],\n",
       "         ...,\n",
       "         [ 0.7467, -1.4994,  6.6481,  ...,  5.7102,  3.1248,  0.9893],\n",
       "         [-2.9425, -5.3104,  5.5955,  ...,  3.1973,  0.4016,  0.6766],\n",
       "         [-3.0447, -3.6674,  2.3292,  ...,  5.3026, -3.7148,  5.0062]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.enc_dec_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1507e+01,  3.6942e+00,  1.1896e+00,  ...,  1.6182e+00,\n",
       "           3.6384e+00,  2.1935e+00],\n",
       "         [ 1.0845e+01,  6.0303e+00, -7.7406e-01,  ...,  1.4043e-01,\n",
       "           3.0258e+00,  8.1023e-01],\n",
       "         [ 9.9115e+00,  4.8149e+00, -4.1417e+00,  ...,  5.3367e+00,\n",
       "           1.2551e+00, -3.6881e+00],\n",
       "         ...,\n",
       "         [ 7.6631e+00,  6.2149e+00, -9.9306e-01,  ...,  2.7145e+00,\n",
       "           8.3932e-01, -4.0012e+00],\n",
       "         [ 1.2607e+01,  5.7130e+00,  3.9997e+00,  ..., -2.6609e+00,\n",
       "           3.6242e+00, -1.5606e-03],\n",
       "         [ 8.1767e+00,  8.1239e+00, -4.1463e+00,  ..., -1.4101e+00,\n",
       "          -1.9761e+00,  3.0817e+00]],\n",
       "\n",
       "        [[ 8.9693e+00,  1.9953e+00,  1.9182e+00,  ...,  9.4867e-01,\n",
       "          -8.4640e+00, -8.9125e-01],\n",
       "         [ 1.0684e+01,  6.0985e+00, -2.4769e+00,  ...,  2.7514e+00,\n",
       "          -7.4103e+00, -2.5511e+00],\n",
       "         [ 7.2011e+00,  6.4578e-01,  5.0101e+00,  ...,  4.2513e+00,\n",
       "          -5.8103e+00, -2.3295e+00],\n",
       "         ...,\n",
       "         [ 9.2791e+00,  3.6507e+00, -7.3489e-01,  ..., -2.8437e+00,\n",
       "          -5.5651e+00,  1.2080e+00],\n",
       "         [ 1.0139e+01,  4.7204e+00,  1.5420e+00,  ...,  1.6141e+00,\n",
       "          -4.9544e+00, -3.9497e-01],\n",
       "         [ 7.7880e+00, -2.2527e-01,  2.7985e+00,  ...,  3.4242e+00,\n",
       "          -5.7882e+00, -2.2815e+00]],\n",
       "\n",
       "        [[ 8.5760e+00,  2.4982e+00, -2.9813e+00,  ...,  5.0742e+00,\n",
       "          -2.2817e+00,  7.5227e-01],\n",
       "         [ 9.1094e+00, -4.1372e+00, -3.0349e+00,  ...,  5.1075e+00,\n",
       "           2.6883e+00,  9.9746e-02],\n",
       "         [ 8.7752e+00,  3.6485e+00,  8.9637e-01,  ...,  1.8819e+00,\n",
       "          -4.6092e+00, -1.9250e+00],\n",
       "         ...,\n",
       "         [ 1.0696e+01,  2.2812e+00, -6.3333e+00,  ...,  1.7066e+00,\n",
       "          -1.9854e+00, -1.6879e+00],\n",
       "         [ 1.3827e+01,  1.5591e+00, -2.2393e+00,  ...,  2.2294e+00,\n",
       "          -4.2165e+00, -7.5593e+00],\n",
       "         [ 8.7093e+00,  6.4895e-01, -3.9633e+00,  ...,  2.4943e+00,\n",
       "          -2.0308e+00, -2.4789e+00]],\n",
       "\n",
       "        [[ 6.6428e+00,  7.8174e+00, -1.4987e+00,  ...,  6.3084e-01,\n",
       "           1.1410e+00,  4.7927e-01],\n",
       "         [ 9.2973e+00,  2.8473e+00, -4.7037e+00,  ...,  1.6104e+00,\n",
       "          -1.2969e+00, -1.6207e+00],\n",
       "         [ 1.0275e+01,  7.8295e-01, -6.4197e-01,  ...,  1.1161e+00,\n",
       "          -1.0240e+00, -1.3963e+00],\n",
       "         ...,\n",
       "         [ 1.0861e+01,  6.4000e+00, -3.5458e+00,  ..., -1.1209e+00,\n",
       "          -2.7840e+00, -2.1777e+00],\n",
       "         [ 6.1524e+00, -6.4122e-02, -7.5149e+00,  ...,  1.8000e+00,\n",
       "          -2.3379e+00,  1.8147e+00],\n",
       "         [ 1.0899e+01,  3.3802e+00, -4.2215e+00,  ...,  1.0136e+00,\n",
       "          -5.0583e+00,  1.8226e+00]],\n",
       "\n",
       "        [[ 8.7762e+00,  8.0204e+00, -2.3623e+00,  ...,  1.8535e+00,\n",
       "          -2.9161e+00,  1.3964e+00],\n",
       "         [ 1.3441e+01,  5.5909e+00,  5.6084e-01,  ...,  4.6485e+00,\n",
       "          -3.4592e+00, -1.1309e+00],\n",
       "         [ 8.2177e+00,  5.8642e+00, -1.1824e+00,  ...,  4.2025e+00,\n",
       "           6.8411e-01, -2.2500e+00],\n",
       "         ...,\n",
       "         [ 6.8787e+00,  1.1518e-02, -3.9965e+00,  ...,  5.6299e+00,\n",
       "           9.9133e-01, -5.2834e+00],\n",
       "         [ 8.2117e+00,  6.2778e+00, -3.5779e+00,  ...,  5.5538e+00,\n",
       "          -2.2558e+00,  1.8432e+00],\n",
       "         [ 3.5783e+00,  6.4460e+00, -4.1505e-01,  ...,  7.6634e+00,\n",
       "          -2.7809e+00, -4.0462e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
